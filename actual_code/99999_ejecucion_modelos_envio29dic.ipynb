{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.preprocess import treat_dataset_pandas_init, build_preprocessing_pipeline,INIT_NUMERICAL_COLS, numerical_features\n",
    "\n",
    "from utils.metrics import rmsle_metric\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "from sklearn.ensemble import StackingRegressor, HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from utils.categoricals import CategoricalToStringTransformer, CategoricalEncoder, CatogoricalUnknownTransformer\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"playground-series-s4e12/train.csv\")#.sample(frac=0.1)\n",
    "test_data = pd.read_csv(\"playground-series-s4e12/test.csv\")\n",
    "\n",
    "treated_dataset = treat_dataset_pandas_init(train_data, process_as_category=True)\n",
    "treated_dataset_test = treat_dataset_pandas_init(test_data, process_as_category=True)\n",
    "\n",
    "non_loged_train_local, non_loged_test_local = joblib.load(\"cat_non_loged_local.pkl\")\n",
    "non_loged_train, non_loged_test = joblib.load(\"cat_non_loged.pkl\")\n",
    "\n",
    "treated_dataset['non_log_oof_prediction'] = non_loged_train\n",
    "treated_dataset['non_log_oof_prediction_local'] = non_loged_train_local\n",
    "treated_dataset_test['non_log_oof_prediction'] = non_loged_test\n",
    "treated_dataset_test['non_log_oof_prediction_local'] = non_loged_test_local\n",
    "\n",
    "X_train = treated_dataset.drop(columns=[\"Premium Amount\"])\n",
    "y_train = treated_dataset[\"Premium Amount\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a 60-40 split\n",
    "#splitter = ShuffleSplit(n_splits=5, test_size=0.2, random_state=1)\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {'learning_rate': 0.04656560304624032, 'max_depth': 10, 'min_child_weight': 50, 'subsample': 0.9730484625285342, 'colsample_bytree': 0.5743561244230219, 'gamma': 7.9578377008338235, 'lambda': 9.268384283962487, 'alpha': 9.9663808717552, 'n_estimators': 754}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:18:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.0236011483923748, Test: 1.0314171277936783\n",
      "Starting\n",
      "Train: 1.0236405075379336, Test: 1.0308962014307577\n",
      "Starting\n",
      "Train: 1.0233155884810634, Test: 1.0327360956412754\n",
      "Starting\n",
      "Train: 1.024433375779329, Test: 1.0296840045463045\n",
      "Starting\n",
      "Train: 1.0234516441967898, Test: 1.0319384861214653\n"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in splitter.split(X_train):\n",
    "    print(\"Starting\")\n",
    "    # Prepare data for LightGBM\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    y_train_fold = np.log1p(y_train_fold)\n",
    "    y_val_fold = np.log1p(y_val_fold)\n",
    "    \n",
    "    full_pipeline = Pipeline([\n",
    "        #(\"categorical_encoder\", CategoricalEncoder()),\n",
    "        (\"model\",xgb.XGBRegressor(enable_categorical=True, **xgboost_params, device=\"cuda\", tree_method=\"hist\"))\n",
    "    ])\n",
    "    full_pipeline.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_train_fold)), np.expm1(y_train_fold))\n",
    "    val_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_val_fold)), np.expm1(y_val_fold))\n",
    "\n",
    "    print(f\"Train: {train_score}, Test: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(enable_categorical=True, **xgboost_params, device=\"cuda\", tree_method=\"hist\")\n",
    "xgb_model.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "test_predictions = np.expm1(xgb_model.predict(treated_dataset_test))\n",
    "test_predictions = np.clip(test_predictions, 20, 4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\": test_data[\"id\"], \"Premium Amount\": test_predictions})\n",
    "submission.to_csv(\"submission_xgb_local.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
