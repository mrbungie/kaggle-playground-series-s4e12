{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.preprocess import treat_dataset_pandas_init, build_preprocessing_pipeline,INIT_NUMERICAL_COLS, numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"playground-series-s4e12/train.csv\")\n",
    "test_data = pd.read_csv(\"playground-series-s4e12/test.csv\")\n",
    "\n",
    "treated_dataset = treat_dataset_pandas_init(train_data, target_transform=\"log1p\", process_as_category=True)\n",
    "\n",
    "X_train = treated_dataset.drop(columns=[\"Premium Amount\"])\n",
    "y_train = treated_dataset[\"Premium Amount\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Premium Amount\"] = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import source_range_root_mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.metrics import make_scorer\n",
    "ag_rmsle_scorer = make_scorer(name='rmsle',\n",
    "                                 score_func=source_range_root_mean_squared_log_error,\n",
    "                                 optimum=0,\n",
    "                                 greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          7.962067\n",
       "1          7.302496\n",
       "2          6.342121\n",
       "3          6.641182\n",
       "4          7.612337\n",
       "             ...   \n",
       "1199995    7.173192\n",
       "1199996    6.711740\n",
       "1199997    5.918894\n",
       "1199998    6.391917\n",
       "1199999    7.816417\n",
       "Name: Premium Amount, Length: 1200000, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Premium Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Health Score</th>\n",
       "      <th>Location</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Policy Start Year_null</th>\n",
       "      <th>Policy Start Month_null</th>\n",
       "      <th>Policy Start Day_null</th>\n",
       "      <th>Policy Start Hour_null</th>\n",
       "      <th>Policy Start Minute_null</th>\n",
       "      <th>Policy Start Second_null</th>\n",
       "      <th>Annual Income log 10_null</th>\n",
       "      <th>Previous Claims log_null</th>\n",
       "      <th>Null columns</th>\n",
       "      <th>Premium Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>10049.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>22.598761</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>7.962067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>31678.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.569731</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>7.302496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>25602.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>47.177549</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>6.342121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>141855.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.938144</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Basic</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>6.641182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>39651.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>20.376094</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>7.612337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199995</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>27316.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>13.772907</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>7.173192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199996</th>\n",
       "      <td>54.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>35786.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>11.483482</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>6.711740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199997</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>51884.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.724469</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Basic</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>5.918894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199998</th>\n",
       "      <td>55.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.547381</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>6.391917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199999</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.125323</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>7.816417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200000 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
       "0        19.0  Female        10049.0        Married                   1.0   \n",
       "1        39.0  Female        31678.0       Divorced                   3.0   \n",
       "2        23.0    Male        25602.0       Divorced                   3.0   \n",
       "3        21.0    Male       141855.0        Married                   2.0   \n",
       "4        21.0    Male        39651.0         Single                   1.0   \n",
       "...       ...     ...            ...            ...                   ...   \n",
       "1199995  36.0  Female        27316.0        Married                   0.0   \n",
       "1199996  54.0    Male        35786.0       Divorced                   NaN   \n",
       "1199997  19.0    Male        51884.0       Divorced                   0.0   \n",
       "1199998  55.0    Male            NaN         Single                   1.0   \n",
       "1199999  21.0  Female            NaN       Divorced                   0.0   \n",
       "\n",
       "        Education Level     Occupation  Health Score  Location    Policy Type  \\\n",
       "0            Bachelor's  Self-Employed     22.598761     Urban        Premium   \n",
       "1              Master's            NaN     15.569731     Rural  Comprehensive   \n",
       "2           High School  Self-Employed     47.177549  Suburban        Premium   \n",
       "3            Bachelor's            NaN     10.938144     Rural          Basic   \n",
       "4            Bachelor's  Self-Employed     20.376094     Rural        Premium   \n",
       "...                 ...            ...           ...       ...            ...   \n",
       "1199995        Master's     Unemployed     13.772907     Urban        Premium   \n",
       "1199996        Master's  Self-Employed     11.483482     Rural  Comprehensive   \n",
       "1199997        Master's            NaN     14.724469  Suburban          Basic   \n",
       "1199998             PhD            NaN     18.547381  Suburban        Premium   \n",
       "1199999             PhD            NaN     10.125323     Rural        Premium   \n",
       "\n",
       "         ...  Policy Start Year_null  Policy Start Month_null  \\\n",
       "0        ...                   False                    False   \n",
       "1        ...                   False                    False   \n",
       "2        ...                   False                    False   \n",
       "3        ...                   False                    False   \n",
       "4        ...                   False                    False   \n",
       "...      ...                     ...                      ...   \n",
       "1199995  ...                   False                    False   \n",
       "1199996  ...                   False                    False   \n",
       "1199997  ...                   False                    False   \n",
       "1199998  ...                   False                    False   \n",
       "1199999  ...                   False                    False   \n",
       "\n",
       "         Policy Start Day_null  Policy Start Hour_null  \\\n",
       "0                        False                   False   \n",
       "1                        False                   False   \n",
       "2                        False                   False   \n",
       "3                        False                   False   \n",
       "4                        False                   False   \n",
       "...                        ...                     ...   \n",
       "1199995                  False                   False   \n",
       "1199996                  False                   False   \n",
       "1199997                  False                   False   \n",
       "1199998                  False                   False   \n",
       "1199999                  False                   False   \n",
       "\n",
       "        Policy Start Minute_null Policy Start Second_null  \\\n",
       "0                          False                    False   \n",
       "1                          False                    False   \n",
       "2                          False                    False   \n",
       "3                          False                    False   \n",
       "4                          False                    False   \n",
       "...                          ...                      ...   \n",
       "1199995                    False                    False   \n",
       "1199996                    False                    False   \n",
       "1199997                    False                    False   \n",
       "1199998                    False                    False   \n",
       "1199999                    False                    False   \n",
       "\n",
       "        Annual Income log 10_null Previous Claims log_null  Null columns  \\\n",
       "0                           False                    False             0   \n",
       "1                           False                    False             1   \n",
       "2                           False                    False             1   \n",
       "3                           False                    False             1   \n",
       "4                           False                    False             0   \n",
       "...                           ...                      ...           ...   \n",
       "1199995                     False                     True             3   \n",
       "1199996                     False                     True             4   \n",
       "1199997                     False                    False             2   \n",
       "1199998                      True                    False             3   \n",
       "1199999                      True                    False             3   \n",
       "\n",
       "         Premium Amount  \n",
       "0              7.962067  \n",
       "1              7.302496  \n",
       "2              6.342121  \n",
       "3              6.641182  \n",
       "4              7.612337  \n",
       "...                 ...  \n",
       "1199995        7.173192  \n",
       "1199996        6.711740  \n",
       "1199997        5.918894  \n",
       "1199998        6.391917  \n",
       "1199999        7.816417  \n",
       "\n",
       "[1200000 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20241227_055104\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       11.89 GB / 31.91 GB (37.3%)\n",
      "Disk Space Avail:   119.93 GB / 931.01 GB (12.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 9900s of the 39600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-12-27 02:51:06,917\tINFO worker.py:1819 -- Started a local Ray instance.\n",
      "\t\tContext path: \"d:\\Development\\insurance\\NEW_FIRST_TRY\\AutogluonModels\\ag-20241227_055104\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Beginning AutoGluon training ... Time limit = 9896s\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m AutoGluon will save models to \"d:\\Development\\insurance\\NEW_FIRST_TRY\\AutogluonModels\\ag-20241227_055104\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Train Data Rows:    1066666\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Train Data Columns: 58\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Label Column:       Premium Amount\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tAvailable Memory:                    11220.47 MB\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTrain Data (Original)  Memory Usage: 169.88 MB (1.5% of available memory)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t\tNote: Converting 16 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tUseless Original Features (Count: 18): ['Policy Start Hour', 'Policy Start Minute', 'Policy Start Second', 'Gender_null', 'Education Level_null', 'Location_null', 'Policy Type_null', 'Smoking Status_null', 'Exercise Frequency_null', 'Property Type_null', 'Premium Amount_null', 'Customer Tenure_null', 'Policy Start Year_null', 'Policy Start Month_null', 'Policy Start Day_null', 'Policy Start Hour_null', 'Policy Start Minute_null', 'Policy Start Second_null']\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tUnused Original Features (Count: 4): ['Insurance Duration_null', 'Claim Frequency_null', 'Annual Income log 10_null', 'Previous Claims log_null']\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('bool', []) : 4 | ['Insurance Duration_null', 'Claim Frequency_null', 'Annual Income log 10_null', 'Previous Claims log_null']\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('bool', [])     : 10 | ['Age_null', 'Annual Income_null', 'Marital Status_null', 'Number of Dependents_null', 'Occupation_null', ...]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('category', []) : 10 | ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location', ...]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('float', [])    : 12 | ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', ...]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('int', [])      :  4 | ['Policy Start Year', 'Policy Start Month', 'Policy Start Day', 'Null columns']\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('category', [])  :  8 | ['Marital Status', 'Education Level', 'Occupation', 'Location', 'Policy Type', ...]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('float', [])     : 12 | ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', ...]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('int', [])       :  4 | ['Policy Start Year', 'Policy Start Month', 'Policy Start Day', 'Null columns']\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\t('int', ['bool']) : 12 | ['Gender', 'Smoking Status', 'Age_null', 'Annual Income_null', 'Marital Status_null', ...]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.5s = Fit runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t36 features in original data used to generate 36 features in processed data.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTrain Data (Processed) Memory Usage: 138.35 MB (1.2% of available memory)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Data preprocessing and feature engineering runtime = 2.62s ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'rmsle'\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ... Tuning model for up to 54.95s of the 9893.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for KNeighborsUnif_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 679.57s compared to 70.86s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused KNeighborsUnif_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: KNeighborsDist_BAG_L1 ... Tuning model for up to 54.95s of the 9890.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for KNeighborsDist_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 583.56s compared to 70.83s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused KNeighborsDist_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 54.95s of the 9889.53s of remaining time.\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=8.68%)\n",
      "\u001b[36m(_ray_fit pid=35200)\u001b[0m \tRan out of time, early stopping on iteration 314. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=35200)\u001b[0m \t[314]\tvalid_set's l2: 1.09504\tvalid_set's rmsle: -1.04644\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/40 [00:44<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBMXT_BAG_L1\\T1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0491\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t44.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t13.86s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 54.95s of the 9844.28s of remaining time.\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=8.42%)\n",
      "\u001b[36m(_ray_fit pid=35828)\u001b[0m \tRan out of time, early stopping on iteration 389. Best iteration is:\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35828)\u001b[0m \t[366]\tvalid_set's l2: 1.0871\tvalid_set's rmsle: -1.04264\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/40 [00:44<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_BAG_L1\\T1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0456\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t44.18s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t9.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForestMSE_BAG_L1 ... Tuning model for up to 54.95s of the 9798.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForestMSE_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 1160.6s to train, which exceeds the maximum time limit of 54.6s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForestMSE_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 54.95s of the 9781.67s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=31984)\u001b[0m \tRan out of time, early stopping on iteration 372. Best iteration is:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=31984)\u001b[0m \t[328]\tvalid_set's l2: 1.08845\tvalid_set's rmsle: -1.04329\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=8.76%)\n",
      "\u001b[36m(_ray_fit pid=20168)\u001b[0m Warning: Large model size may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=20168)\u001b[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\u001b[36m(_ray_fit pid=20168)\u001b[0m Available Memory: 808 MB, Estimated Model size: 892 MB\n",
      "\u001b[36m(_ray_fit pid=20168)\u001b[0m \tRan low on memory, early stopping on iteration 5.\n",
      "\u001b[36m(_ray_fit pid=35752)\u001b[0m \tRan out of time, early stopping on iteration 15.\n",
      "\u001b[36m(_ray_fit pid=29384)\u001b[0m Warning: Large model size may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=29384)\u001b[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\u001b[36m(_ray_fit pid=29384)\u001b[0m Available Memory: 832 MB, Estimated Model size: 892 MB\n",
      "\u001b[36m(_ray_fit pid=29384)\u001b[0m \tRan low on memory, early stopping on iteration 5.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/40 [00:39<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_BAG_L1\\T1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0727\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t39.79s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTreesMSE_BAG_L1 ... Tuning model for up to 54.95s of the 9741.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTreesMSE_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 497.4s to train, which exceeds the maximum time limit of 54.5s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTreesMSE_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ... Tuning model for up to 54.95s of the 9732.85s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=22764)\u001b[0m \tRan out of time, early stopping on iteration 16.\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetFastAI_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler            │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                       │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=36932)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.41% memory usage per fold, 57.63%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=21116)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(model_trial pid=33084)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.28% memory usage per fold, 69.13%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=33676)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=33676)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=31944)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=34776)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=34280)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1' in 0.0090s.\n",
      "\u001b[36m(_ray_fit pid=21116)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 2949c785: FileNotFoundError('Could not fetch metrics for 2949c785: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/2949c785')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 0143a2fc: FileNotFoundError('Could not fetch metrics for 0143a2fc: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/0143a2fc')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 999063bf: FileNotFoundError('Could not fetch metrics for 999063bf: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/999063bf')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetFastAI_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_BAG_L1 ... Tuning model for up to 54.95s of the 9666.63s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=5700)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.14% memory usage per fold, 56.29%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=28.14%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/40 [00:52<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_BAG_L1\\T1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0623\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t51.96s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_BAG_L1 ... Tuning model for up to 54.95s of the 9613.01s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=27832)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator         │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                      │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=20484)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=20484)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_ray_fit pid=30896)\u001b[0m \tNot enough time to train first epoch. (Time Required: 14.62s, Time Left: 13.57s)\n",
      "\u001b[36m(model_trial pid=39732)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.17% memory usage per fold, 44.70%/80.00% total).\n",
      "\u001b[36m(model_trial pid=26508)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1' in 0.0090s.\n",
      "\u001b[36m(model_trial pid=19416)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.63% memory usage per fold, 54.52%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - b89ce129: FileNotFoundError('Could not fetch metrics for b89ce129: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/b89ce129')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - ef3da26f: FileNotFoundError('Could not fetch metrics for ef3da26f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/ef3da26f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 18dd1c87: FileNotFoundError('Could not fetch metrics for 18dd1c87: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/18dd1c87')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 47ea0772: FileNotFoundError('Could not fetch metrics for 47ea0772: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/47ea0772')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9cff34b5: FileNotFoundError('Could not fetch metrics for 9cff34b5: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/9cff34b5')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 54.95s of the 9553.50s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.60% memory usage per fold, 62.40%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.60%)\n",
      "\u001b[36m(_ray_fit pid=30564)\u001b[0m \tRan out of time, early stopping on iteration 326. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=30564)\u001b[0m \t[298]\tvalid_set's l2: 1.09215\tvalid_set's rmsle: -1.04506\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0455\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t44.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t4.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r177_BAG_L1 ... Tuning model for up to 54.95s of the 9505.86s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=3104)\u001b[0m \tRan out of time, early stopping on iteration 323. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3104)\u001b[0m \t[287]\tvalid_set's l2: 1.09898\tvalid_set's rmsle: -1.04832\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r177_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=14.04%)\n",
      "\u001b[36m(_ray_fit pid=36624)\u001b[0m \tRan out of time, early stopping on iteration 271.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r177_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.048\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t45.92s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r79_BAG_L1 ... Tuning model for up to 54.95s of the 9459.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r79_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=36632)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.91% memory usage per fold, 59.65%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=39832)\u001b[0m \tNot enough time to train first epoch. (Time Required: 10.98s, Time Left: 9.63s)\n",
      "\u001b[36m(_ray_fit pid=32340)\u001b[0m \tRan out of time, early stopping on iteration 274.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(model_trial pid=36632)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.16% memory usage per fold, 40.32%/80.00% total).\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=21076, ip=127.0.0.1)\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 224, in _fit\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(model_trial pid=37520)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(model_trial pid=39860)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.94% memory usage per fold, 43.88%/80.00% total).\n",
      "\u001b[36m(model_trial pid=39084)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.99% memory usage per fold, 49.97%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1' in 0.0070s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 0b9c1946: FileNotFoundError('Could not fetch metrics for 0b9c1946: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/0b9c1946')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 8139a101: FileNotFoundError('Could not fetch metrics for 8139a101: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/8139a101')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 36a3bd19: FileNotFoundError('Could not fetch metrics for 36a3bd19: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/36a3bd19')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 19b81e0e: FileNotFoundError('Could not fetch metrics for 19b81e0e: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/19b81e0e')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - ac6d8062: FileNotFoundError('Could not fetch metrics for ac6d8062: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/ac6d8062')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r79_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r131_BAG_L1 ... Tuning model for up to 54.95s of the 9402.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r131_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.08% memory usage per fold, 66.16%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=33.08%)\n",
      "\u001b[36m(_ray_fit pid=39508)\u001b[0m \tRan out of time, early stopping on iteration 244. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=39508)\u001b[0m \t[244]\tvalid_set's l2: 1.0921\tvalid_set's rmsle: -1.04503\n",
      "\u001b[36m(_ray_fit pid=35676)\u001b[0m \tRan out of time, early stopping on iteration 304. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35676)\u001b[0m \t[304]\tvalid_set's l2: 1.08734\tvalid_set's rmsle: -1.04276\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35692)\u001b[0m \tRan out of time, early stopping on iteration 457. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35692)\u001b[0m \t[457]\tvalid_set's l2: 1.09726\tvalid_set's rmsle: -1.0475\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r131_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0463\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t52.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.88s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r191_BAG_L1 ... Tuning model for up to 54.95s of the 9349.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r191_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 67.23% memory usage per fold, 67.23%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=67.23%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=5180)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 365, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2753, in get\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 904, in get_objects\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ray.exceptions.RayTaskError(TimeLimitExceeded): \u001b[36mray::_ray_fit()\u001b[39m (pid=5180, ip=127.0.0.1)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 338, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 298, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 573, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r9_BAG_L1 ... Tuning model for up to 54.95s of the 9332.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r9_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.57% memory usage per fold, 71.14%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=35.57%)\n",
      "\u001b[36m(_ray_fit pid=39368)\u001b[0m \tRan out of time, early stopping on iteration 7.\n",
      "\u001b[36m(_ray_fit pid=27720)\u001b[0m Warning: Low available memory may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=27720)\u001b[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\u001b[36m(_ray_fit pid=27720)\u001b[0m Available Memory: 477 MB, Estimated Model size: 428 MB\n",
      "\u001b[36m(_ray_fit pid=27720)\u001b[0m \tRan low on memory, early stopping on iteration 6.\n",
      "\u001b[36m(_ray_fit pid=36948)\u001b[0m \tRan out of time, early stopping on iteration 8.\n",
      "\u001b[36m(_ray_fit pid=36100)\u001b[0m Warning: Large model size may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=36100)\u001b[0m Warning: Low available memory may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=36100)\u001b[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\u001b[36m(_ray_fit pid=36100)\u001b[0m Available Memory: 378 MB, Estimated Model size: 429 MB\n",
      "\u001b[36m(_ray_fit pid=36100)\u001b[0m \tRan low on memory, early stopping on iteration 2.\n",
      "\u001b[36m(_ray_fit pid=35680)\u001b[0m \tRan out of time, early stopping on iteration 36.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r9_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0765\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t39.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.65s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r96_BAG_L1 ... Tuning model for up to 54.95s of the 9291.25s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r96_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.31% memory usage per fold, 58.63%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=29.31%)\n",
      "\u001b[36m(_ray_fit pid=38836)\u001b[0m \tRan out of time, early stopping on iteration 220. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=38836)\u001b[0m \t[220]\tvalid_set's l2: 1.12146\tvalid_set's rmsle: -1.05899\n",
      "\u001b[36m(_ray_fit pid=34844)\u001b[0m \tRan out of time, early stopping on iteration 214. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=34844)\u001b[0m \t[214]\tvalid_set's l2: 1.11618\tvalid_set's rmsle: -1.05649\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30452)\u001b[0m \tRan out of time, early stopping on iteration 39. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30452)\u001b[0m \t[39]\tvalid_set's l2: 1.17664\tvalid_set's rmsle: -1.08473\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r96_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0653\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t56.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.46s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r22_BAG_L1 ... Tuning model for up to 54.95s of the 9233.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r22_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=37800)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.40% memory usage per fold, 42.81%/80.00% total).\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=5128, ip=127.0.0.1)\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 224, in _fit\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(model_trial pid=37800)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(model_trial pid=28392)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.34% memory usage per fold, 48.68%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1' in 0.0335s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - a5b2c991: FileNotFoundError('Could not fetch metrics for a5b2c991: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/a5b2c991')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 89243120: FileNotFoundError('Could not fetch metrics for 89243120: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/89243120')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 05d45b4b: FileNotFoundError('Could not fetch metrics for 05d45b4b: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/05d45b4b')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r22_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r33_BAG_L1 ... Tuning model for up to 54.95s of the 9177.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r33_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.64% memory usage per fold, 73.28%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=36.64%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m hyperparameter_tune_kwargs \u001b[38;5;241m=\u001b[39m {  \n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_trials\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearcher\u001b[39m\u001b[38;5;124m'\u001b[39m  : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      7\u001b[0m predictor \u001b[38;5;241m=\u001b[39m TabularPredictor(label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPremium Amount\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                              eval_metric \u001b[38;5;241m=\u001b[39m ag_rmsle_scorer,\n\u001b[0;32m      9\u001b[0m                              problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m                             )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 10 minutes\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpresets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_quality\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m              \u001b[49m\u001b[43msave_space\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m              \u001b[49m\u001b[43mkeep_only_best\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mgargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgkwargs)\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1280\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_stacking:\n\u001b[0;32m   1275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDyStack is enabled (dynamic_stacking=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynamic_stacking\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m     )\n\u001b[1;32m-> 1280\u001b[0m     num_stack_levels, time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_stacking(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mds_args, ag_fit_kwargs\u001b[38;5;241m=\u001b[39mag_fit_kwargs, ag_post_fit_kwargs\u001b[38;5;241m=\u001b[39mag_post_fit_kwargs)\n\u001b[0;32m   1281\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting main fit with num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mFor future fit calls on this dataset, you can skip DyStack to save time: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`predictor.fit(..., dynamic_stacking=False, num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m     )\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (time_limit \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1380\u001b[0m, in \u001b[0;36mTabularPredictor._dynamic_stacking\u001b[1;34m(self, ag_fit_kwargs, ag_post_fit_kwargs, validation_procedure, detection_time_frac, holdout_frac, n_folds, n_repeats, memory_safe_fits, clean_up_fits, enable_ray_logging, enable_callbacks, holdout_data)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         _, holdout_data, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_data(train_data\u001b[38;5;241m=\u001b[39mX, tuning_data\u001b[38;5;241m=\u001b[39mholdout_data)\n\u001b[0;32m   1378\u001b[0m         ds_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds_fit_context\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ds_fit_context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_fit_custom_ho\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1380\u001b[0m     stacked_overfitting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sub_fit_memory_save_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;66;03m# Holdout is false, use (repeated) cross-validation\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m     is_stratified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;129;01min\u001b[39;00m [REGRESSION, QUANTILE, SOFTCLASS]\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1563\u001b[0m, in \u001b[0;36mTabularPredictor._sub_fit_memory_save_wrapper\u001b[1;34m(self, train_data, time_limit, time_start, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# FIXME: For some reason ray does not treat `num_cpus` and `num_gpus` the same.\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m#  For `num_gpus`, the process will reserve the capacity and is unable to share it to child ray processes, causing a deadlock.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;66;03m#  For `num_cpus`, the value is completely ignored by children, and they can even use more num_cpus than the parent.\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m#  Because of this, num_gpus is set to 0 here to avoid a deadlock, but num_cpus does not need to be changed.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;66;03m#  For more info, refer to Ray documentation: https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html#yielding-resources-while-blocked\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m ref \u001b[38;5;241m=\u001b[39m sub_fit_caller\u001b[38;5;241m.\u001b[39moptions(num_cpus\u001b[38;5;241m=\u001b[39mnum_cpus, num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mremote(\n\u001b[0;32m   1555\u001b[0m     predictor\u001b[38;5;241m=\u001b[39mpredictor_ref,\n\u001b[0;32m   1556\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_data_ref,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     holdout_data\u001b[38;5;241m=\u001b[39mholdout_data_ref,\n\u001b[0;32m   1562\u001b[0m )\n\u001b[1;32m-> 1563\u001b[0m finished, unfinished \u001b[38;5;241m=\u001b[39m \u001b[43m_ds_ray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m stacked_overfitting, ho_leaderboard, exception \u001b[38;5;241m=\u001b[39m _ds_ray\u001b[38;5;241m.\u001b[39mget(finished[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;66;03m# TODO: This is present to ensure worker logs are properly logged and don't get skipped / printed out of order.\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;66;03m#  Ideally find a faster way to do this that doesn't introduce a 100 ms overhead.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     20\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py:2984\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[0;32m   2982\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m   2983\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m-> 2984\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:3816\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\includes/common.pxi:79\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r33_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0907\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t66.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r42_BAG_L1 ... Tuning model for up to 54.95s of the 9108.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r42_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 221 due to low memory. Expected memory usage reduced from 20.31% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 358.7s to train, which exceeds the maximum time limit of 54.4s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r42_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r137_BAG_L1 ... Tuning model for up to 54.95s of the 9100.67s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r137_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 23.67% memory usage per fold, 47.35%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=23.67%)\n",
      "\u001b[36m(_ray_fit pid=28384)\u001b[0m \tRan out of time, early stopping on iteration 13.\n",
      "\u001b[36m(_ray_fit pid=19684)\u001b[0m \tRan out of time, early stopping on iteration 35.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=39320)\u001b[0m \tRan out of time, early stopping on iteration 21.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r137_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0731\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t75.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t1.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r102_BAG_L1 ... Tuning model for up to 54.95s of the 9023.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r102_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 41.88% memory usage per fold, 41.88%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=41.88%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=35776)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=35776)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=19816)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=19816)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=34580)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 365, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2753, in get\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 904, in get_objects\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ray.exceptions.RayTaskError(TimeLimitExceeded): \u001b[36mray::_ray_fit()\u001b[39m (pid=34580, ip=127.0.0.1)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 338, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 298, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 573, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r13_BAG_L1 ... Tuning model for up to 54.95s of the 8932.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r13_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.83% memory usage per fold, 41.66%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=20.83%)\n",
      "\u001b[36m(_ray_fit pid=32476)\u001b[0m \tRan out of time, early stopping on iteration 86.\n",
      "\u001b[36m(_ray_fit pid=14196)\u001b[0m \tRan out of time, early stopping on iteration 1.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18316)\u001b[0m \tRan out of time, early stopping on iteration 90.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r13_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0708\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t70.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t1.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r195_BAG_L1 ... Tuning model for up to 54.95s of the 8860.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r195_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 135 due to low memory. Expected memory usage reduced from 33.14% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 694.4s to train, which exceeds the maximum time limit of 54.3s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r195_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r188_BAG_L1 ... Tuning model for up to 54.95s of the 8838.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r188_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.96% memory usage per fold, 41.93%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=20.96%)\n",
      "\u001b[36m(_ray_fit pid=32824)\u001b[0m \tRan out of time, early stopping on iteration 246. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=32824)\u001b[0m \t[246]\tvalid_set's l2: 1.10023\tvalid_set's rmsle: -1.04892\n",
      "\u001b[36m(_ray_fit pid=25432)\u001b[0m \tRan out of time, early stopping on iteration 18. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=25432)\u001b[0m \t[18]\tvalid_set's l2: 1.16322\tvalid_set's rmsle: -1.07853\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28476)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28476)\u001b[0m \t[1]\tvalid_set's l2: 1.20068\tvalid_set's rmsle: -1.09576\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r188_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0696\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t61.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t1.84s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r145_BAG_L1 ... Tuning model for up to 54.95s of the 8774.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r145_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.10% memory usage per fold, 72.20%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=36.10%)\n",
      "\u001b[36m(_ray_fit pid=34004)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=26788)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=26788)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=28780)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=34004)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 365, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2753, in get\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 904, in get_objects\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ray.exceptions.RayTaskError(TimeLimitExceeded): \u001b[36mray::_ray_fit()\u001b[39m (pid=38096, ip=127.0.0.1)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 338, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 298, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 573, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r89_BAG_L1 ... Tuning model for up to 54.95s of the 8539.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r89_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=12.50%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r89_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0692\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t50.28s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r30_BAG_L1 ... Tuning model for up to 54.95s of the 8487.73s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=38096)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r30_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=18768)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.68% memory usage per fold, 50.72%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=36044)\u001b[0m \tNot enough time to train first epoch. (Time Required: 29.25s, Time Left: 19.49s)\n",
      "\u001b[36m(model_trial pid=18768)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=18768)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=18768)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=32556)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.08% memory usage per fold, 52.34%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1' in 0.0120s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - add5fc60: FileNotFoundError('Could not fetch metrics for add5fc60: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/add5fc60')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - eb2e32f8: FileNotFoundError('Could not fetch metrics for eb2e32f8: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/eb2e32f8')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 0d591506: FileNotFoundError('Could not fetch metrics for 0d591506: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/0d591506')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - bec26735: FileNotFoundError('Could not fetch metrics for bec26735: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/bec26735')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 620425e5: FileNotFoundError('Could not fetch metrics for 620425e5: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/620425e5')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r30_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r130_BAG_L1 ... Tuning model for up to 54.95s of the 8420.97s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r130_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=10.34%)\n",
      "\u001b[36m(_ray_fit pid=18200)\u001b[0m \tRan out of time, early stopping on iteration 189. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=18200)\u001b[0m \t[188]\tvalid_set's l2: 1.08651\tvalid_set's rmsle: -1.04236\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r130_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0459\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t51.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t5.99s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r86_BAG_L1 ... Tuning model for up to 54.95s of the 8368.74s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r86_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=3104)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.90% memory usage per fold, 47.59%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=28716)\u001b[0m \tRan out of time, early stopping on iteration 181. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28716)\u001b[0m \t[181]\tvalid_set's l2: 1.0898\tvalid_set's rmsle: -1.04394\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(model_trial pid=38956)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=38956)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=38956)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=38956)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=38956)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=3104)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=39460)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.22% memory usage per fold, 40.90%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1' in 0.0710s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 5122ac8f: FileNotFoundError('Could not fetch metrics for 5122ac8f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/5122ac8f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - fb11b8a9: FileNotFoundError('Could not fetch metrics for fb11b8a9: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/fb11b8a9')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 009090df: FileNotFoundError('Could not fetch metrics for 009090df: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/009090df')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 8f9cf528: FileNotFoundError('Could not fetch metrics for 8f9cf528: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/8f9cf528')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - dfaa61e7: FileNotFoundError('Could not fetch metrics for dfaa61e7: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/dfaa61e7')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r86_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r50_BAG_L1 ... Tuning model for up to 54.95s of the 8305.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r50_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=10.29%)\n",
      "\u001b[36m(_ray_fit pid=22344)\u001b[0m \tRan out of time, early stopping on iteration 103.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r50_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0521\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t46.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.47s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r11_BAG_L1 ... Tuning model for up to 54.95s of the 8258.0s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r11_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.36% memory usage per fold, 65.42%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=16.36%)\n",
      "\u001b[36m(_ray_fit pid=19292)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=15636)\u001b[0m \tRan out of time, early stopping on iteration 103.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38804)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=38804)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19864)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1584)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=1584)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r11_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0634\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t504.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t135.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r194_BAG_L1 ... Tuning model for up to 54.95s of the 7752.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r194_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=10.38%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r194_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0458\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t48.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t1.89s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r172_BAG_L1 ... Tuning model for up to 54.95s of the 7703.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r172_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 747.4s to train, which exceeds the maximum time limit of 54.3s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r172_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r69_BAG_L1 ... Tuning model for up to 54.95s of the 7691.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r69_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.51%)\n",
      "\u001b[36m(_ray_fit pid=23636)\u001b[0m \tRan out of time, early stopping on iteration 25.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r69_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0663\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t48.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r103_BAG_L1 ... Tuning model for up to 54.95s of the 7641.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r103_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=13.10%)\n",
      "\u001b[36m(_ray_fit pid=28732)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=32460)\u001b[0m \tRan out of time, early stopping on iteration 26.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28732)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39256)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r103_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0555\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t93.27s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t16.74s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r14_BAG_L1 ... Tuning model for up to 54.95s of the 7546.61s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=39256)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r14_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=25452)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.45% memory usage per fold, 53.80%/80.00% total).\n",
      "\u001b[36m(model_trial pid=25452)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=28824)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.30% memory usage per fold, 57.21%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=39720)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -0.2s)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1' in 0.0100s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - b36ec40d: FileNotFoundError('Could not fetch metrics for b36ec40d: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1/b36ec40d')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 31bb5ecc: FileNotFoundError('Could not fetch metrics for 31bb5ecc: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1/31bb5ecc')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - e65d079b: FileNotFoundError('Could not fetch metrics for e65d079b: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1/e65d079b')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 15db6678: FileNotFoundError('Could not fetch metrics for 15db6678: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1/15db6678')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 00b742fe: FileNotFoundError('Could not fetch metrics for 00b742fe: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r14_BAG_L1/00b742fe')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r14_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r161_BAG_L1 ... Tuning model for up to 54.95s of the 7483.9s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=22264)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -0.2s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(model_trial pid=36572)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=36572)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=36572)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=36572)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=36572)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r161_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=11.80%)\n",
      "\u001b[36m(_ray_fit pid=30804)\u001b[0m \tRan out of time, early stopping on iteration 136. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=30804)\u001b[0m \t[136]\tvalid_set's l2: 1.12209\tvalid_set's rmsle: -1.05929\n",
      "\u001b[36m(_ray_fit pid=20756)\u001b[0m \tRan out of time, early stopping on iteration 78. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=20756)\u001b[0m \t[78]\tvalid_set's l2: 1.13457\tvalid_set's rmsle: -1.06516\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r161_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0648\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t54.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.78s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r143_BAG_L1 ... Tuning model for up to 54.95s of the 7427.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r143_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.46% memory usage per fold, 77.86%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=19.46%)\n",
      "\u001b[36m(_ray_fit pid=36924)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=17916)\u001b[0m \tRan out of time, early stopping on iteration 73. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17916)\u001b[0m \t[73]\tvalid_set's l2: 1.13522\tvalid_set's rmsle: -1.06547\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=36924)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=30572)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30572)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=26652)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=26652)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r143_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0626\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t123.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t5.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r70_BAG_L1 ... Tuning model for up to 54.95s of the 7303.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r70_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=12.51%)\n",
      "\u001b[36m(_ray_fit pid=26740)\u001b[0m \tRan out of time, early stopping on iteration 51.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r70_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0513\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t59.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.64s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r156_BAG_L1 ... Tuning model for up to 54.95s of the 7243.23s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=36596)\u001b[0m \tRan out of time, early stopping on iteration 52.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r156_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.18% memory usage per fold, 42.36%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=21.18%)\n",
      "\u001b[36m(_ray_fit pid=31472)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=31472)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=4604)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=4912)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=4604)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=4912)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=15524)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39904)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=15524)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39904)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r156_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0746\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t148.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t9.42s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r196_BAG_L1 ... Tuning model for up to 54.95s of the 7093.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r196_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=12.75%)\n",
      "\u001b[36m(_ray_fit pid=39404)\u001b[0m \tRan out of time, early stopping on iteration 180. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=39404)\u001b[0m \t[180]\tvalid_set's l2: 1.16944\tvalid_set's rmsle: -1.08141\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r196_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0772\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t51.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t14.99s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r39_BAG_L1 ... Tuning model for up to 54.95s of the 7041.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r39_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_ray_fit pid=14996)\u001b[0m \tRan out of time, early stopping on iteration 174. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14996)\u001b[0m \t[174]\tvalid_set's l2: 1.166\tvalid_set's rmsle: -1.07982\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 1007.9s to train, which exceeds the maximum time limit of 54.3s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r39_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r167_BAG_L1 ... Tuning model for up to 54.95s of the 7026.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r167_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.87%)\n",
      "\u001b[36m(_ray_fit pid=29996)\u001b[0m \tRan out of time, early stopping on iteration 24.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r167_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0537\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t57.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r95_BAG_L1 ... Tuning model for up to 54.95s of the 6967.08s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r95_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=13.43%)\n",
      "\u001b[36m(_ray_fit pid=22536)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=19424)\u001b[0m \tRan out of time, early stopping on iteration 24.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38080)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=22904)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r95_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0566\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t182.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t32.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r41_BAG_L1 ... Tuning model for up to 54.95s of the 6782.74s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=29556)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r41_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r41_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=18268)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.54% memory usage per fold, 42.17%/80.00% total).\n",
      "\u001b[36m(model_trial pid=18268)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=18268)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=18268)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=34800)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.23% memory usage per fold, 44.93%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r41_BAG_L1' in 0.0065s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 3d84e5cf: FileNotFoundError('Could not fetch metrics for 3d84e5cf: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r41_BAG_L1/3d84e5cf')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 33744148: FileNotFoundError('Could not fetch metrics for 33744148: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r41_BAG_L1/33744148')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - df683e0f: FileNotFoundError('Could not fetch metrics for df683e0f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r41_BAG_L1/df683e0f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r41_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r98_BAG_L1 ... Tuning model for up to 54.95s of the 6716.22s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r98_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 23.29% memory usage per fold, 46.57%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=23.29%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r98_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0913\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t70.48s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.58s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r15_BAG_L1 ... Tuning model for up to 54.95s of the 6643.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r15_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=11.59%)\n",
      "\u001b[36m(_ray_fit pid=1348)\u001b[0m \tRan out of time, early stopping on iteration 264. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1348)\u001b[0m \t[264]\tvalid_set's l2: 1.09107\tvalid_set's rmsle: -1.04454\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r15_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0464\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t48.99s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t5.79s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r158_BAG_L1 ... Tuning model for up to 54.95s of the 6593.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r158_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator              │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler                │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r158_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=10608)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.79% memory usage per fold, 51.14%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=4976)\u001b[0m \tRan out of time, early stopping on iteration 269. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4976)\u001b[0m \t[269]\tvalid_set's l2: 1.09468\tvalid_set's rmsle: -1.04627\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(model_trial pid=10608)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=10608)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r158_BAG_L1' in 0.0443s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 204f26f4: FileNotFoundError('Could not fetch metrics for 204f26f4: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r158_BAG_L1/204f26f4')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9d01aca2: FileNotFoundError('Could not fetch metrics for 9d01aca2: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r158_BAG_L1/9d01aca2')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - b39d4fea: FileNotFoundError('Could not fetch metrics for b39d4fea: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r158_BAG_L1/b39d4fea')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r158_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r86_BAG_L1 ... Tuning model for up to 54.95s of the 6526.8s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r86_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=13.47%)\n",
      "\u001b[36m(_ray_fit pid=24200)\u001b[0m \tRan out of time, early stopping on iteration 12.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r86_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0738\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t47.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r37_BAG_L1 ... Tuning model for up to 54.95s of the 6478.15s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r37_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.92% memory usage per fold, 49.84%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=24.92%)\n",
      "\u001b[36m(_ray_fit pid=10220)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=34952)\u001b[0m \tRan out of time, early stopping on iteration 13.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38904)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=38904)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=27568)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=10220)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=37704)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=37704)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=31600)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=27568)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=31600)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r37_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0551\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t277.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t9.42s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r197_BAG_L1 ... Tuning model for up to 54.95s of the 6199.92s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r197_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator              │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler                │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=29356)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.61% memory usage per fold, 58.45%/80.00% total).\n",
      "\u001b[36m(model_trial pid=29356)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=39028)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.53% memory usage per fold, 46.13%/80.00% total).\n",
      "\u001b[36m(model_trial pid=38488)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.82% memory usage per fold, 67.27%/80.00% total).\n",
      "\u001b[36m(_ray_fit pid=7988)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -0.5s)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1' in 0.0111s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - f49124d4: FileNotFoundError('Could not fetch metrics for f49124d4: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1/f49124d4')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 7dfaab3d: FileNotFoundError('Could not fetch metrics for 7dfaab3d: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1/7dfaab3d')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9feee3cd: FileNotFoundError('Could not fetch metrics for 9feee3cd: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1/9feee3cd')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 484ae8bb: FileNotFoundError('Could not fetch metrics for 484ae8bb: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1/484ae8bb')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 796184ee: FileNotFoundError('Could not fetch metrics for 796184ee: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r197_BAG_L1/796184ee')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r197_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r49_BAG_L1 ... Tuning model for up to 54.95s of the 6141.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r49_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=12.90%)\n",
      "\u001b[36m(_ray_fit pid=25456)\u001b[0m \tRan out of time, early stopping on iteration 289.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r49_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0517\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t46.97s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.26s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r49_BAG_L1 ... Tuning model for up to 54.95s of the 6093.55s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r49_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 271 due to low memory. Expected memory usage reduced from 16.56% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 271 -> 82 due to low time. Expected time usage reduced from 176.7s -> 54.3s...\n",
      "\u001b[36m(_ray_fit pid=13432)\u001b[0m \tRan out of time, early stopping on iteration 292.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 1251.0s compared to 51.33s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r49_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r143_BAG_L1 ... Tuning model for up to 54.95s of the 6071.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r143_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=13.14%)\n",
      "\u001b[36m(_ray_fit pid=8780)\u001b[0m \tRan out of time, early stopping on iteration 201. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8780)\u001b[0m \t[201]\tvalid_set's l2: 1.09061\tvalid_set's rmsle: -1.04432\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r143_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0461\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t57.84s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t8.41s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r127_BAG_L1 ... Tuning model for up to 54.95s of the 6012.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r127_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 171 due to low memory. Expected memory usage reduced from 26.31% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 884.5s to train, which exceeds the maximum time limit of 54.4s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r127_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r134_BAG_L1 ... Tuning model for up to 54.95s of the 5989.82s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=23108)\u001b[0m \tRan out of time, early stopping on iteration 203. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=23108)\u001b[0m \t[203]\tvalid_set's l2: 1.09768\tvalid_set's rmsle: -1.0477\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r134_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.33% memory usage per fold, 54.66%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=27.33%)\n",
      "\u001b[36m(_ray_fit pid=11192)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=11192)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=19712)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=19712)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=25868)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=25868)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=1576)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=1576)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=13052)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=13052)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r134_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0683\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t247.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t34.6s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r34_BAG_L1 ... Tuning model for up to 54.95s of the 5740.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r34_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 1080.6s to train, which exceeds the maximum time limit of 54.7s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r34_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r94_BAG_L1 ... Tuning model for up to 54.95s of the 5725.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r94_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.71% memory usage per fold, 70.82%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=17.71%)\n",
      "\u001b[36m(_ray_fit pid=35172)\u001b[0m \tRan out of time, early stopping on iteration 33. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=35172)\u001b[0m \t[33]\tvalid_set's l2: 1.15772\tvalid_set's rmsle: -1.07598\n",
      "\u001b[36m(_ray_fit pid=26992)\u001b[0m \tRan out of time, early stopping on iteration 72. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=26992)\u001b[0m \t[72]\tvalid_set's l2: 1.14629\tvalid_set's rmsle: -1.07065\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r94_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0758\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t57.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t1.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r143_BAG_L1 ... Tuning model for up to 54.95s of the 5665.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r143_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator              │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler                │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r143_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=32264)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.49% memory usage per fold, 69.95%/80.00% total).\n",
      "\u001b[36m(model_trial pid=32264)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r143_BAG_L1' in 0.0071s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 8f06b435: FileNotFoundError('Could not fetch metrics for 8f06b435: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r143_BAG_L1/8f06b435')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - c0c0de4c: FileNotFoundError('Could not fetch metrics for c0c0de4c: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r143_BAG_L1/c0c0de4c')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - a23e3b2e: FileNotFoundError('Could not fetch metrics for a23e3b2e: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r143_BAG_L1/a23e3b2e')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r143_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r128_BAG_L1 ... Tuning model for up to 54.95s of the 5608.84s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r128_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.42% memory usage per fold, 58.84%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=29.42%)\n",
      "\u001b[36m(_ray_fit pid=29820)\u001b[0m \tRan out of time, early stopping on iteration 35.\n",
      "\u001b[36m(_ray_fit pid=23032)\u001b[0m \tRan out of time, early stopping on iteration 45.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10168)\u001b[0m \tRan out of time, early stopping on iteration 145.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r128_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.05\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t55.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r111_BAG_L1 ... Tuning model for up to 54.95s of the 5552.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r111_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.65% memory usage per fold, 73.29%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=36.65%)\n",
      "\u001b[36m(_ray_fit pid=29044)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=29044)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=29540)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=34804)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=29540)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=35312)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=35312)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=32000)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=34804)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=32000)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r111_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0609\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t49.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r31_BAG_L1 ... Tuning model for up to 54.95s of the 5502.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r31_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=1520)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.10% memory usage per fold, 44.20%/80.00% total).\n",
      "\u001b[36m(model_trial pid=28972)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.86% memory usage per fold, 55.72%/80.00% total).\n",
      "\u001b[36m(model_trial pid=33268)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.89% memory usage per fold, 67.79%/80.00% total).\n",
      "\u001b[36m(model_trial pid=23060)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=23060)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=23060)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 55.48% memory usage per fold, 55.48%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9af78677: FileNotFoundError('Could not fetch metrics for 9af78677: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1/9af78677')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 01c0cb9b: FileNotFoundError('Could not fetch metrics for 01c0cb9b: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1/01c0cb9b')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 72cd66df: FileNotFoundError('Could not fetch metrics for 72cd66df: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1/72cd66df')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 713b065f: FileNotFoundError('Could not fetch metrics for 713b065f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1/713b065f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 64c4f8e2: FileNotFoundError('Could not fetch metrics for 64c4f8e2: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r31_BAG_L1/64c4f8e2')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r31_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r4_BAG_L1 ... Tuning model for up to 54.95s of the 5446.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r4_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 106 due to low memory. Expected memory usage reduced from 42.24% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 151.8s to train, which exceeds the maximum time limit of 54.7s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r4_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r65_BAG_L1 ... Tuning model for up to 54.95s of the 5440.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r65_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Potentially not enough memory to safely train model. Estimated to require 1.707 GB out of 2.001 GB available memory (85.311%)... (90.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.19 to avoid the warning)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 85.31% memory usage per fold, 85.31%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=85.31%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=22428)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=22428)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=8444)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=8444)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=15948)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=15948)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=20532)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=20532)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=18828)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=18828)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r65_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0879\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t72.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r88_BAG_L1 ... Tuning model for up to 54.95s of the 5366.49s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r88_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Potentially not enough memory to safely train model. Estimated to require 1.707 GB out of 2.040 GB available memory (83.682%)... (90.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.17 to avoid the warning)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 83.68% memory usage per fold, 83.68%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=83.68%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=8568)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=8568)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=7056)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=7056)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=13556)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=13556)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=37416)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=37416)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=33184)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=33184)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r88_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0702\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t70.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r30_BAG_L1 ... Tuning model for up to 54.95s of the 5295.59s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r30_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 46.26% memory usage per fold, 46.26%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=46.26%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=8964)\u001b[0m \tRan out of time, early stopping on iteration 257. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8964)\u001b[0m \t[257]\tvalid_set's l2: 1.10694\tvalid_set's rmsle: -1.05211\n",
      "\u001b[36m(_ray_fit pid=26604)\u001b[0m \tRan out of time, early stopping on iteration 256. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=26604)\u001b[0m \t[256]\tvalid_set's l2: 1.11843\tvalid_set's rmsle: -1.05756\n",
      "\u001b[36m(_ray_fit pid=2396)\u001b[0m \tRan out of time, early stopping on iteration 253. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2396)\u001b[0m \t[253]\tvalid_set's l2: 1.10264\tvalid_set's rmsle: -1.05006\n",
      "\u001b[36m(_ray_fit pid=19556)\u001b[0m \tRan out of time, early stopping on iteration 259. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=19556)\u001b[0m \t[259]\tvalid_set's l2: 1.11024\tvalid_set's rmsle: -1.05368\n",
      "\u001b[36m(_ray_fit pid=39476)\u001b[0m \tRan out of time, early stopping on iteration 250. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=39476)\u001b[0m \t[250]\tvalid_set's l2: 1.11325\tvalid_set's rmsle: -1.05511\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r30_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0537\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t56.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r49_BAG_L1 ... Tuning model for up to 54.95s of the 5238.8s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r49_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 59.94% memory usage per fold, 59.94%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=59.94%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r49_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0787\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t55.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r5_BAG_L1 ... Tuning model for up to 54.95s of the 5182.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r5_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 41.51% memory usage per fold, 41.51%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=41.51%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=37868)\u001b[0m \tRan out of time, early stopping on iteration 197.\n",
      "\u001b[36m(_ray_fit pid=4712)\u001b[0m \tRan out of time, early stopping on iteration 230.\n",
      "\u001b[36m(_ray_fit pid=27272)\u001b[0m \tRan out of time, early stopping on iteration 234.\n",
      "\u001b[36m(_ray_fit pid=4684)\u001b[0m \tRan out of time, early stopping on iteration 223.\n",
      "\u001b[36m(_ray_fit pid=12268)\u001b[0m \tRan out of time, early stopping on iteration 241.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r5_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0529\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t52.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r87_BAG_L1 ... Tuning model for up to 54.95s of the 5130.17s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r87_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=7332)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 48.34% memory usage per fold, 48.34%/80.00% total).\n",
      "\u001b[36m(model_trial pid=7332)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=7332)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=38444)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 51.31% memory usage per fold, 51.31%/80.00% total).\n",
      "\u001b[36m(model_trial pid=38444)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=38444)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=34060)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.85% memory usage per fold, 43.85%/80.00% total).\n",
      "\u001b[36m(model_trial pid=34060)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=34060)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=18928)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=18928)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=18928)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 51.84% memory usage per fold, 51.84%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 6 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 56edd3d2: FileNotFoundError('Could not fetch metrics for 56edd3d2: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1/56edd3d2')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 5d70fbd3: FileNotFoundError('Could not fetch metrics for 5d70fbd3: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1/5d70fbd3')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - ed6667a6: FileNotFoundError('Could not fetch metrics for ed6667a6: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1/ed6667a6')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9909d790: FileNotFoundError('Could not fetch metrics for 9909d790: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1/9909d790')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 1feebb63: FileNotFoundError('Could not fetch metrics for 1feebb63: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1/1feebb63')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - fd336417: FileNotFoundError('Could not fetch metrics for fd336417: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r87_BAG_L1/fd336417')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r87_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r71_BAG_L1 ... Tuning model for up to 54.95s of the 5074.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r71_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=34004)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 48.07% memory usage per fold, 48.07%/80.00% total).\n",
      "\u001b[36m(model_trial pid=34004)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=34004)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=26032)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 52.87% memory usage per fold, 52.87%/80.00% total).\n",
      "\u001b[36m(model_trial pid=26032)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=26032)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=36088)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 49.76% memory usage per fold, 49.76%/80.00% total).\n",
      "\u001b[36m(model_trial pid=36088)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=36088)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=8092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=8092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=8092)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 44.96% memory usage per fold, 44.96%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 1a311560: FileNotFoundError('Could not fetch metrics for 1a311560: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1/1a311560')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 55836e0f: FileNotFoundError('Could not fetch metrics for 55836e0f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1/55836e0f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - ded5987b: FileNotFoundError('Could not fetch metrics for ded5987b: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1/ded5987b')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 6e8035a9: FileNotFoundError('Could not fetch metrics for 6e8035a9: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1/6e8035a9')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 924ea515: FileNotFoundError('Could not fetch metrics for 924ea515: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r71_BAG_L1/924ea515')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r71_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r143_BAG_L1 ... Tuning model for up to 54.95s of the 5017.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r143_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 45.48% memory usage per fold, 45.48%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=45.48%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=38060)\u001b[0m \tRan out of time, early stopping on iteration 128.\n",
      "\u001b[36m(_ray_fit pid=34436)\u001b[0m \tRan out of time, early stopping on iteration 124.\n",
      "\u001b[36m(_ray_fit pid=39632)\u001b[0m \tRan out of time, early stopping on iteration 139.\n",
      "\u001b[36m(_ray_fit pid=35160)\u001b[0m \tRan out of time, early stopping on iteration 144.\n",
      "\u001b[36m(_ray_fit pid=28404)\u001b[0m \tRan out of time, early stopping on iteration 133.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r143_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0485\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t51.99s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r178_BAG_L1 ... Tuning model for up to 54.95s of the 4965.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r178_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 82 due to low memory. Expected memory usage reduced from 54.73% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 82 -> 50 due to low time. Expected time usage reduced from 87.9s -> 54.7s...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 64.0s compared to 35.85s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r178_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r166_BAG_L1 ... Tuning model for up to 54.95s of the 4928.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r166_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 50 due to low memory. Expected memory usage reduced from 88.34% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 64.0s compared to 57.61s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r166_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r31_BAG_L1 ... Tuning model for up to 54.95s of the 4914.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r31_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 65.07% memory usage per fold, 65.07%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=65.07%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r31_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0917\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t54.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r185_BAG_L1 ... Tuning model for up to 54.95s of the 4859.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r185_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator              │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler                │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=26540)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.26% memory usage per fold, 42.26%/80.00% total).\n",
      "\u001b[36m(model_trial pid=26540)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=26540)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=20012)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.75% memory usage per fold, 42.75%/80.00% total).\n",
      "\u001b[36m(model_trial pid=20012)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=20012)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=14420)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 45.80% memory usage per fold, 45.80%/80.00% total).\n",
      "\u001b[36m(model_trial pid=14420)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=14420)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=39852)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 44.94% memory usage per fold, 44.94%/80.00% total).\n",
      "\u001b[36m(model_trial pid=39852)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=39852)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 6 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - ba2ae98f: FileNotFoundError('Could not fetch metrics for ba2ae98f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1/ba2ae98f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 0f77ea20: FileNotFoundError('Could not fetch metrics for 0f77ea20: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1/0f77ea20')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - b36f4352: FileNotFoundError('Could not fetch metrics for b36f4352: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1/b36f4352')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9579c90a: FileNotFoundError('Could not fetch metrics for 9579c90a: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1/9579c90a')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 7e6f2f23: FileNotFoundError('Could not fetch metrics for 7e6f2f23: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1/7e6f2f23')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - be308c28: FileNotFoundError('Could not fetch metrics for be308c28: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r185_BAG_L1/be308c28')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r185_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r160_BAG_L1 ... Tuning model for up to 54.95s of the 4803.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r160_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Potentially not enough memory to safely train model. Estimated to require 1.707 GB out of 2.181 GB available memory (78.246%)... (90.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.09 to avoid the warning)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 78.25% memory usage per fold, 78.25%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=78.25%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=26108)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=26108)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=25544)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=25544)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=7056)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=7056)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=5052)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=5052)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=31512)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=31512)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r160_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0582\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t280.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t17.42s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r60_BAG_L1 ... Tuning model for up to 54.95s of the 4522.26s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r60_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 38.03% memory usage per fold, 76.06%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=38.03%)\n",
      "\u001b[36m(_ray_fit pid=34580)\u001b[0m Warning: Large model size may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=34580)\u001b[0m Warning: Low available memory may cause OOM error if training continues\n",
      "\u001b[36m(_ray_fit pid=34580)\u001b[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\u001b[36m(_ray_fit pid=34580)\u001b[0m Available Memory: 399 MB, Estimated Model size: 556 MB\n",
      "\u001b[36m(_ray_fit pid=34580)\u001b[0m \tRan low on memory, early stopping on iteration 2.\n",
      "\u001b[36m(_ray_fit pid=16716)\u001b[0m Warning: Large model size may cause OOM error if training continues\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16716)\u001b[0m Warning: Low available memory may cause OOM error if training continues\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16716)\u001b[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16716)\u001b[0m Available Memory: 473 MB, Estimated Model size: 617 MB\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16716)\u001b[0m \tRan low on memory, early stopping on iteration 3.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15940)\u001b[0m \tRan out of time, early stopping on iteration 54.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r60_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0837\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t19.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r15_BAG_L1 ... Tuning model for up to 54.95s of the 4501.88s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r15_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 98 due to low memory. Expected memory usage reduced from 45.58% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 255.6s to train, which exceeds the maximum time limit of 54.7s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r15_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r135_BAG_L1 ... Tuning model for up to 54.95s of the 4490.48s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r135_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.27% memory usage per fold, 60.54%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=30.27%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r135_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0459\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t35.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t1.77s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r22_BAG_L1 ... Tuning model for up to 54.95s of the 4454.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r22_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.81% memory usage per fold, 73.63%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=36.81%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r22_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0667\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t52.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r69_BAG_L1 ... Tuning model for up to 54.95s of the 4401.28s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r69_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 41.39% memory usage per fold, 41.39%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=41.39%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=6016)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=6016)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=35892)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=35892)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=31556)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=31556)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=30920)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=30920)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=18340)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=18340)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r69_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0565\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t266.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t16.86s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r6_BAG_L1 ... Tuning model for up to 54.95s of the 4133.67s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r6_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.25% memory usage per fold, 48.49%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=24.25%)\n",
      "\u001b[36m(_ray_fit pid=39048)\u001b[0m \tRan out of time, early stopping on iteration 114.\n",
      "\u001b[36m(_ray_fit pid=29012)\u001b[0m \tRan out of time, early stopping on iteration 113.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=25804)\u001b[0m \tRan out of time, early stopping on iteration 169.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r6_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0514\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t48.97s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.33s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r138_BAG_L1 ... Tuning model for up to 54.95s of the 4084.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r138_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.92% memory usage per fold, 42.92%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=42.92%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=18880)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=18880)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39344)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39344)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=18188)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=18188)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=22608)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=22608)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=13688)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=13688)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r138_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0667\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t292.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t18.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r121_BAG_L1 ... Tuning model for up to 54.95s of the 3791.31s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r121_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.11% memory usage per fold, 52.22%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=26.11%)\n",
      "\u001b[36m(_ray_fit pid=18036)\u001b[0m \tRan out of time, early stopping on iteration 285. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=18036)\u001b[0m \t[285]\tvalid_set's l2: 1.09512\tvalid_set's rmsle: -1.04648\n",
      "\u001b[36m(_ray_fit pid=26156)\u001b[0m \tRan out of time, early stopping on iteration 286. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=26156)\u001b[0m \t[286]\tvalid_set's l2: 1.09828\tvalid_set's rmsle: -1.04799\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28216)\u001b[0m \tRan out of time, early stopping on iteration 375. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28216)\u001b[0m \t[375]\tvalid_set's l2: 1.09908\tvalid_set's rmsle: -1.04837\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: LightGBM_r121_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.048\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t52.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t4.71s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r172_BAG_L1 ... Tuning model for up to 54.95s of the 3738.0s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r172_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.60% memory usage per fold, 42.60%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=42.60%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=38484)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=38484)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=25684)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=25684)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=33420)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=33420)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=20920)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=20920)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=26072)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=26072)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r172_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0732\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t93.0s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t4.98s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r180_BAG_L1 ... Tuning model for up to 54.95s of the 3644.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r180_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.28% memory usage per fold, 50.56%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=25.28%)\n",
      "\u001b[36m(_ray_fit pid=3404)\u001b[0m \tRan out of time, early stopping on iteration 122.\n",
      "\u001b[36m(_ray_fit pid=25256)\u001b[0m \tRan out of time, early stopping on iteration 122.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=26204)\u001b[0m \tRan out of time, early stopping on iteration 200.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r180_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0469\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t49.0s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r76_BAG_L1 ... Tuning model for up to 54.95s of the 3594.66s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r76_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=27936)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.08% memory usage per fold, 54.17%/80.00% total).\n",
      "\u001b[36m(model_trial pid=35748)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.91% memory usage per fold, 63.82%/80.00% total).\n",
      "\u001b[36m(model_trial pid=792)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.30% memory usage per fold, 43.30%/80.00% total).\n",
      "\u001b[36m(model_trial pid=792)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=792)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=4768)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.70% memory usage per fold, 43.70%/80.00% total).\n",
      "\u001b[36m(model_trial pid=4768)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=4768)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 6 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - d5a29ad7: FileNotFoundError('Could not fetch metrics for d5a29ad7: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1/d5a29ad7')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 5f85254d: FileNotFoundError('Could not fetch metrics for 5f85254d: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1/5f85254d')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - c5c586ac: FileNotFoundError('Could not fetch metrics for c5c586ac: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1/c5c586ac')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 3d248c7b: FileNotFoundError('Could not fetch metrics for 3d248c7b: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1/3d248c7b')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - c77418d9: FileNotFoundError('Could not fetch metrics for c77418d9: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1/c77418d9')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 0c20945f: FileNotFoundError('Could not fetch metrics for 0c20945f: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r76_BAG_L1/0c20945f')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r76_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r197_BAG_L1 ... Tuning model for up to 54.95s of the 3538.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r197_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 70 due to low memory. Expected memory usage reduced from 64.04% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 70 -> 41 due to low time. Expected time usage reduced from 93.0s -> 54.7s...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 42.67s compared to 33.2s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r197_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r121_BAG_L1 ... Tuning model for up to 54.95s of the 3499.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r121_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator              │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler                │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=33948)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.93% memory usage per fold, 43.93%/80.00% total).\n",
      "\u001b[36m(model_trial pid=33948)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=33948)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=32092)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.72% memory usage per fold, 43.72%/80.00% total).\n",
      "\u001b[36m(model_trial pid=32092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=32092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=12312)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.86% memory usage per fold, 42.86%/80.00% total).\n",
      "\u001b[36m(model_trial pid=12312)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=12312)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=15468)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.25% memory usage per fold, 42.25%/80.00% total).\n",
      "\u001b[36m(model_trial pid=15468)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=15468)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 6 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - de89a2e9: FileNotFoundError('Could not fetch metrics for de89a2e9: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1/de89a2e9')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 73622bdc: FileNotFoundError('Could not fetch metrics for 73622bdc: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1/73622bdc')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - d4981392: FileNotFoundError('Could not fetch metrics for d4981392: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1/d4981392')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 276596d2: FileNotFoundError('Could not fetch metrics for 276596d2: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1/276596d2')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 8d622cac: FileNotFoundError('Could not fetch metrics for 8d622cac: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1/8d622cac')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - e3034470: FileNotFoundError('Could not fetch metrics for e3034470: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r121_BAG_L1/e3034470')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r121_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r127_BAG_L1 ... Tuning model for up to 54.95s of the 3444.16s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r127_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 67.80% memory usage per fold, 67.80%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=67.80%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=39000)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39000)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=26156)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=26156)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39616)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39616)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=22724)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=22724)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=13688)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=13688)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r127_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0562\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t70.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.38s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: RandomForest_r16_BAG_L1 ... Tuning model for up to 54.95s of the 3373.4s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for RandomForest_r16_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 57 due to low memory. Expected memory usage reduced from 78.09% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Model is expected to require 204.5s to train, which exceeds the maximum time limit of 54.7s, skipping model...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused RandomForest_r16_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 506, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r194_BAG_L1 ... Tuning model for up to 54.95s of the 3358.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r194_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 65.10% memory usage per fold, 65.10%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=65.10%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=11004)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=11004)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=21808)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=21808)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39404)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39404)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=25476)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=25476)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=34004)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=34004)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r194_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0601\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t167.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t9.85s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r12_BAG_L1 ... Tuning model for up to 54.95s of the 3189.94s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r12_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.43% memory usage per fold, 72.85%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=36.43%)\n",
      "\u001b[36m(_ray_fit pid=25080)\u001b[0m \tRan out of time, early stopping on iteration 198.\n",
      "\u001b[36m(_ray_fit pid=30864)\u001b[0m \tRan out of time, early stopping on iteration 191.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=29504)\u001b[0m \tRan out of time, early stopping on iteration 254.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r12_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0488\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t48.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r135_BAG_L1 ... Tuning model for up to 54.95s of the 3140.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r135_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator              │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler                │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                           │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=100)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 39.93% memory usage per fold, 79.87%/80.00% total).\n",
      "\u001b[36m(model_trial pid=100)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=22536)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.80% memory usage per fold, 67.59%/80.00% total).\n",
      "\u001b[36m(model_trial pid=33360)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.99% memory usage per fold, 49.98%/80.00% total).\n",
      "\u001b[36m(model_trial pid=36812)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.21% memory usage per fold, 60.42%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - b380009a: FileNotFoundError('Could not fetch metrics for b380009a: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1/b380009a')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 7e29d5fd: FileNotFoundError('Could not fetch metrics for 7e29d5fd: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1/7e29d5fd')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - ca97ca2e: FileNotFoundError('Could not fetch metrics for ca97ca2e: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1/ca97ca2e')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 7dc3fcd0: FileNotFoundError('Could not fetch metrics for 7dc3fcd0: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1/7dc3fcd0')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 516da1cb: FileNotFoundError('Could not fetch metrics for 516da1cb: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r135_BAG_L1/516da1cb')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r135_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r4_BAG_L1 ... Tuning model for up to 54.95s of the 3084.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r4_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 60.27% memory usage per fold, 60.27%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=60.27%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=30916)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=30916)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39872)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39872)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=35256)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=35256)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=36580)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=36580)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=21752)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=21752)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r4_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0713\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t138.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t8.46s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: ExtraTrees_r126_BAG_L1 ... Tuning model for up to 54.95s of the 2945.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for ExtraTrees_r126_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 114 due to low memory. Expected memory usage reduced from 39.37% -> 15.0% of available memory...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 64.05s compared to 49.42s of available time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused ExtraTrees_r126_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 273, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 534, in _fit_single\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r36_BAG_L1 ... Tuning model for up to 54.95s of the 2922.63s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r36_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=19920)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.38% memory usage per fold, 64.77%/80.00% total).\n",
      "\u001b[36m(model_trial pid=39316)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 41.67% memory usage per fold, 41.67%/80.00% total).\n",
      "\u001b[36m(model_trial pid=39316)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=39316)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=22556)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.65% memory usage per fold, 40.65%/80.00% total).\n",
      "\u001b[36m(model_trial pid=22556)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=22556)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=37960)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.28% memory usage per fold, 74.57%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1' in 0.0055s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 5 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 2a8c25cb: FileNotFoundError('Could not fetch metrics for 2a8c25cb: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1/2a8c25cb')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 61e30d8c: FileNotFoundError('Could not fetch metrics for 61e30d8c: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1/61e30d8c')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9ebeae2c: FileNotFoundError('Could not fetch metrics for 9ebeae2c: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1/9ebeae2c')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 1d0cceb1: FileNotFoundError('Could not fetch metrics for 1d0cceb1: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1/1d0cceb1')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 8b1a6ef0: FileNotFoundError('Could not fetch metrics for 8b1a6ef0: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r36_BAG_L1/8b1a6ef0')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r36_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r100_BAG_L1 ... Tuning model for up to 54.95s of the 2866.8s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r100_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 1.707 GB out of 1.546 GB available memory (110.379%)... (90.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.28 to avoid the error)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused NeuralNetFastAI_r100_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 365, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 298, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 700, in _fit_folds\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 497, in __init__\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     num_folds_parallel = self.folds_to_fit_in_parallel_with_mem(user_specified_num_folds_parallel=num_folds_parallel)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 520, in folds_to_fit_in_parallel_with_mem\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._initialized_model_base._validate_fit_memory_usage(approx_mem_size_req=mem_est_total, available_mem=mem_available)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 2188, in _validate_fit_memory_usage\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise NotEnoughMemoryError\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.utils.exceptions.NotEnoughMemoryError\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r163_BAG_L1 ... Tuning model for up to 54.95s of the 2865.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r163_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 59.58% memory usage per fold, 59.58%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=59.58%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=6468)\u001b[0m \tRan out of time, early stopping on iteration 61.\n",
      "\u001b[36m(_ray_fit pid=39012)\u001b[0m \tRan out of time, early stopping on iteration 64.\n",
      "\u001b[36m(_ray_fit pid=34156)\u001b[0m \tRan out of time, early stopping on iteration 63.\n",
      "\u001b[36m(_ray_fit pid=27324)\u001b[0m \tRan out of time, early stopping on iteration 61.\n",
      "\u001b[36m(_ray_fit pid=4708)\u001b[0m \tRan out of time, early stopping on iteration 63.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r163_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0527\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t51.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: CatBoost_r198_BAG_L1 ... Tuning model for up to 54.95s of the 2813.33s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r198_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.78% memory usage per fold, 75.57%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=37.78%)\n",
      "\u001b[36m(_ray_fit pid=36012)\u001b[0m \tRan out of time, early stopping on iteration 215.\n",
      "\u001b[36m(_ray_fit pid=8328)\u001b[0m \tRan out of time, early stopping on iteration 220.\n",
      "\u001b[36m(_ray_fit pid=35604)\u001b[0m \tRan out of time, early stopping on iteration 213.\n",
      "\u001b[36m(_ray_fit pid=20576)\u001b[0m \tRan out of time, early stopping on iteration 212.\n",
      "\u001b[36m(_ray_fit pid=34844)\u001b[0m \tRan out of time, early stopping on iteration 280.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: CatBoost_r198_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0493\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t53.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t0.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r187_BAG_L1 ... Tuning model for up to 54.95s of the 2759.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r187_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 65.27% memory usage per fold, 65.27%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=65.27%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=30228)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=30228)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=20192)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=20192)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=39000)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=39000)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=26376)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=26376)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_ray_fit pid=20064)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n",
      "\u001b[36m(_ray_fit pid=20064)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: NeuralNetFastAI_r187_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0593\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t75.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t3.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r19_BAG_L1 ... Tuning model for up to 54.95s of the 2683.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r19_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m │ Number of trials                 40                          │\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m View detailed results here: D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(model_trial pid=23808)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 45.30% memory usage per fold, 45.30%/80.00% total).\n",
      "\u001b[36m(model_trial pid=23808)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=23808)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=20056)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 48.31% memory usage per fold, 48.31%/80.00% total).\n",
      "\u001b[36m(model_trial pid=20056)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=20056)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=20044)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 47.53% memory usage per fold, 47.53%/80.00% total).\n",
      "\u001b[36m(model_trial pid=20044)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=20044)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(model_trial pid=11900)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 47.43% memory usage per fold, 47.43%/80.00% total).\n",
      "\u001b[36m(model_trial pid=11900)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(model_trial pid=11900)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Reached timeout of 54.951753894686696 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Wrote the latest version of all result files and experiment state to 'D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1' in 0.0000s.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Failed to fetch metrics for 6 trial(s):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 9fede57a: FileNotFoundError('Could not fetch metrics for 9fede57a: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1/9fede57a')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 7bd4b16a: FileNotFoundError('Could not fetch metrics for 7bd4b16a: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1/7bd4b16a')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 53c0161d: FileNotFoundError('Could not fetch metrics for 53c0161d: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1/53c0161d')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - a99da9a3: FileNotFoundError('Could not fetch metrics for a99da9a3: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1/a99da9a3')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - 1469454c: FileNotFoundError('Could not fetch metrics for 1469454c: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1/1469454c')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m - d4b58ac5: FileNotFoundError('Could not fetch metrics for d4b58ac5: both result.json and progress.csv were not found at D:/Development/insurance/NEW_FIRST_TRY/AutogluonModels/ag-20241227_055104/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r19_BAG_L1/d4b58ac5')\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r19_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r95_BAG_L1 ... Tuning model for up to 54.95s of the 2626.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r95_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 51.27% memory usage per fold, 51.27%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=51.27%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r95_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0697\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t57.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: XGBoost_r34_BAG_L1 ... Tuning model for up to 54.95s of the 2568.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r34_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 53.68% memory usage per fold, 53.68%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=53.68%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Fitted model: XGBoost_r34_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t-1.0857\t = Validation score   (-rmsle)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t57.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \t2.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: LightGBM_r42_BAG_L1 ... Tuning model for up to 54.95s of the 2509.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r42_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 39.69% memory usage per fold, 79.39%/80.00% total).\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=39.69%)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Warning: Exception caused LightGBM_r42_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1395, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 525, in validate_search_space\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m \n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2482, in _train_single_full\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1751, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 289, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 1397, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 298, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 609, in _run_parallel\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     ref = self._fit(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 715, in _fit\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     fold_ctx_ref = self.ray.put(fold_ctx)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2834, in put\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     object_ref = worker.put_object(value, owner_address=serialize_owner_address)\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"d:\\Development\\insurance\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 816, in put_object\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m     self.core_worker.put_serialized_object_and_increment_local_ref(\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 3742, in ray._raylet.CoreWorker.put_serialized_object_and_increment_local_ref\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 3528, in ray._raylet.CoreWorker._create_put_buffer\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m   File \"python\\ray\\includes/common.pxi\", line 89, in ray._raylet.check_status\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m OSError: Unknown error\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Unknown error\n",
      "\u001b[36m(_dystack pid=37092)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r1_BAG_L1 ... Tuning model for up to 54.95s of the 2506.04s of remaining time.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-27 04:54:20,900 C 29644 11904] (raylet.exe) dlmalloc.cc:129:  Check failed: *handle != nullptr CreateFileMapping() failed. GetLastError() = 1450\n",
      "\u001b[33m(raylet)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[33m(raylet)\u001b[0m unknown\n",
      "\u001b[33m(raylet)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m The node with node id: d4b1b7077d083d117f18e5bb041e2cfd72cf25192d86170c0833b101 and address: 127.0.0.1 and node name: 127.0.0.1 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, etc.) \n",
      "\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_tune_kwargs = {  \n",
    "    'num_trials': 40,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher'  : 'auto',\n",
    "}\n",
    "\n",
    "predictor = TabularPredictor(label = 'Premium Amount',\n",
    "                             eval_metric = ag_rmsle_scorer,\n",
    "                             problem_type = \"regression\",\n",
    "                            )\n",
    "predictor.fit(X_train,\n",
    "              time_limit = 11*60*60, # 10 minutes\n",
    "              hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "              presets = 'best_quality',\n",
    "              save_space = True,\n",
    "              keep_only_best = False,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
