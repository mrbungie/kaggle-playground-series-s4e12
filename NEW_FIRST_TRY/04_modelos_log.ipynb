{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.preprocess import treat_dataset_pandas_init, build_preprocessing_pipeline,INIT_NUMERICAL_COLS, numerical_features\n",
    "\n",
    "from utils.metrics import rmsle_metric\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "from sklearn.ensemble import StackingRegressor, HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from utils.categoricals import CategoricalToStringTransformer, CategoricalEncoder, CatogoricalUnknownTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"playground-series-s4e12/train.csv\")#.sample(frac=0.1)\n",
    "test_data = pd.read_csv(\"playground-series-s4e12/test.csv\")\n",
    "\n",
    "treated_dataset = treat_dataset_pandas_init(train_data, process_as_category=True)\n",
    "#non_loged_train, non_loged_test = joblib.load(\"cat_non_loged_local.pkl\")\n",
    "non_loged_train, non_loged_test = joblib.load(\"cat_non_loged.pkl\")\n",
    "\n",
    "treated_dataset['non_log_oof_prediction'] = non_loged_train\n",
    "\n",
    "X_train = treated_dataset.drop(columns=[\"Premium Amount\"])\n",
    "y_train = treated_dataset[\"Premium Amount\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1408728857820265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_mean_squared_log_error(treated_dataset['Premium Amount'], treated_dataset['non_log_oof_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Health Score</th>\n",
       "      <th>Location</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Income_per_Dependent</th>\n",
       "      <th>CreditScore_InsuranceDuration</th>\n",
       "      <th>Health_Risk_Score</th>\n",
       "      <th>Credit_Health_Score</th>\n",
       "      <th>Health_Age_Interaction</th>\n",
       "      <th>Claims v Duration</th>\n",
       "      <th>Health vs Claims</th>\n",
       "      <th>Cat Credit Score</th>\n",
       "      <th>Int Credit Score</th>\n",
       "      <th>non_log_oof_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>10049.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>22.598761</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>5024.5</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>3.870062</td>\n",
       "      <td>8406.738970</td>\n",
       "      <td>429.376453</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.532920</td>\n",
       "      <td>372.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>1198.816057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>31678.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.569731</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>...</td>\n",
       "      <td>7919.5</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>4.221513</td>\n",
       "      <td>10805.393307</td>\n",
       "      <td>607.219509</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.784865</td>\n",
       "      <td>694.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>953.272016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>25602.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>47.177549</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>6400.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.641123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1085.083634</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>23.588775</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1104.083469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>141855.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.938144</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Basic</td>\n",
       "      <td>...</td>\n",
       "      <td>47285.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>4.453093</td>\n",
       "      <td>4014.298906</td>\n",
       "      <td>229.701027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.469072</td>\n",
       "      <td>367.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>1276.605480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>39651.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>20.376094</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>19825.5</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>3.981195</td>\n",
       "      <td>12184.903989</td>\n",
       "      <td>427.897966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.376094</td>\n",
       "      <td>598.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>1273.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199995</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>27316.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>13.772907</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>27316.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>4.311355</td>\n",
       "      <td>5123.521323</td>\n",
       "      <td>495.824644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>372.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>1234.252578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199996</th>\n",
       "      <td>54.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>35786.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>11.483482</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>4.425826</td>\n",
       "      <td>6855.638903</td>\n",
       "      <td>620.108041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>1213.428470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199997</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>51884.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.724469</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Basic</td>\n",
       "      <td>...</td>\n",
       "      <td>51884.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.263777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>279.764902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.724469</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>544.038304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199998</th>\n",
       "      <td>55.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.547381</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1628.0</td>\n",
       "      <td>4.072631</td>\n",
       "      <td>7548.784101</td>\n",
       "      <td>1020.105960</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>9.273691</td>\n",
       "      <td>407.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>1235.857653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199999</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.125323</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Premium</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3012.0</td>\n",
       "      <td>4.493734</td>\n",
       "      <td>5082.912134</td>\n",
       "      <td>212.631783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.125323</td>\n",
       "      <td>502.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>968.177221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200000 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
       "0        19.0  Female        10049.0        Married                   1.0   \n",
       "1        39.0  Female        31678.0       Divorced                   3.0   \n",
       "2        23.0    Male        25602.0       Divorced                   3.0   \n",
       "3        21.0    Male       141855.0        Married                   2.0   \n",
       "4        21.0    Male        39651.0         Single                   1.0   \n",
       "...       ...     ...            ...            ...                   ...   \n",
       "1199995  36.0  Female        27316.0        Married                   0.0   \n",
       "1199996  54.0    Male        35786.0       Divorced                   NaN   \n",
       "1199997  19.0    Male        51884.0       Divorced                   0.0   \n",
       "1199998  55.0    Male            NaN         Single                   1.0   \n",
       "1199999  21.0  Female            NaN       Divorced                   0.0   \n",
       "\n",
       "        Education Level     Occupation  Health Score  Location    Policy Type  \\\n",
       "0            Bachelor's  Self-Employed     22.598761     Urban        Premium   \n",
       "1              Master's            NaN     15.569731     Rural  Comprehensive   \n",
       "2           High School  Self-Employed     47.177549  Suburban        Premium   \n",
       "3            Bachelor's            NaN     10.938144     Rural          Basic   \n",
       "4            Bachelor's  Self-Employed     20.376094     Rural        Premium   \n",
       "...                 ...            ...           ...       ...            ...   \n",
       "1199995        Master's     Unemployed     13.772907     Urban        Premium   \n",
       "1199996        Master's  Self-Employed     11.483482     Rural  Comprehensive   \n",
       "1199997        Master's            NaN     14.724469  Suburban          Basic   \n",
       "1199998             PhD            NaN     18.547381  Suburban        Premium   \n",
       "1199999             PhD            NaN     10.125323     Rural        Premium   \n",
       "\n",
       "         ...  Income_per_Dependent  CreditScore_InsuranceDuration  \\\n",
       "0        ...                5024.5                         1860.0   \n",
       "1        ...                7919.5                         1388.0   \n",
       "2        ...                6400.5                            NaN   \n",
       "3        ...               47285.0                          367.0   \n",
       "4        ...               19825.5                         2392.0   \n",
       "...      ...                   ...                            ...   \n",
       "1199995  ...               27316.0                         1116.0   \n",
       "1199996  ...                   NaN                         2388.0   \n",
       "1199997  ...               51884.0                            NaN   \n",
       "1199998  ...                   NaN                         1628.0   \n",
       "1199999  ...                   NaN                         3012.0   \n",
       "\n",
       "         Health_Risk_Score  Credit_Health_Score Health_Age_Interaction  \\\n",
       "0                 3.870062          8406.738970             429.376453   \n",
       "1                 4.221513         10805.393307             607.219509   \n",
       "2                 2.641123                  NaN            1085.083634   \n",
       "3                 4.453093          4014.298906             229.701027   \n",
       "4                 3.981195         12184.903989             427.897966   \n",
       "...                    ...                  ...                    ...   \n",
       "1199995           4.311355          5123.521323             495.824644   \n",
       "1199996           4.425826          6855.638903             620.108041   \n",
       "1199997           4.263777                  NaN             279.764902   \n",
       "1199998           4.072631          7548.784101            1020.105960   \n",
       "1199999           4.493734          5082.912134             212.631783   \n",
       "\n",
       "        Claims v Duration Health vs Claims Cat Credit Score  Int Credit Score  \\\n",
       "0                0.400000         7.532920            372.0             372.0   \n",
       "1                0.500000         7.784865            694.0             694.0   \n",
       "2                0.333333        23.588775             <NA>               NaN   \n",
       "3                1.000000         5.469072            367.0             367.0   \n",
       "4                0.000000        20.376094            598.0             598.0   \n",
       "...                   ...              ...              ...               ...   \n",
       "1199995               NaN              NaN            372.0             372.0   \n",
       "1199996               NaN              NaN            597.0             597.0   \n",
       "1199997          0.000000        14.724469             <NA>               NaN   \n",
       "1199998          0.250000         9.273691            407.0             407.0   \n",
       "1199999          0.000000        10.125323            502.0             502.0   \n",
       "\n",
       "         non_log_oof_prediction  \n",
       "0                   1198.816057  \n",
       "1                    953.272016  \n",
       "2                   1104.083469  \n",
       "3                   1276.605480  \n",
       "4                   1273.640800  \n",
       "...                         ...  \n",
       "1199995             1234.252578  \n",
       "1199996             1213.428470  \n",
       "1199997              544.038304  \n",
       "1199998             1235.857653  \n",
       "1199999              968.177221  \n",
       "\n",
       "[1200000 rows x 72 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a 60-40 split\n",
    "#splitter = ShuffleSplit(n_splits=5, test_size=0.2, random_state=1)\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    y_pred = np.maximum(0, y_pred)  # Clip predicted values to be non-negative\n",
    "\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {'learning_rate': 0.04656560304624032, 'max_depth': 10, 'min_child_weight': 50, 'subsample': 0.9730484625285342, 'colsample_bytree': 0.5743561244230219, 'gamma': 7.9578377008338235, 'lambda': 9.268384283962487, 'alpha': 9.9663808717552, 'n_estimators': 754}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:26:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.01508361253977, Test: 1.0336200796724437\n",
      "Starting\n",
      "Train: 1.0153803688262613, Test: 1.032450494995619\n",
      "Starting\n",
      "Train: 1.0148886273675428, Test: 1.034428999931049\n",
      "Starting\n",
      "Train: 1.015938982968538, Test: 1.031844295874394\n",
      "Starting\n",
      "Train: 1.0151394190373497, Test: 1.0336337284126291\n"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in splitter.split(X_train):\n",
    "    print(\"Starting\")\n",
    "    # Prepare data for LightGBM\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    y_train_fold = np.log1p(y_train_fold)\n",
    "    y_val_fold = np.log1p(y_val_fold)\n",
    "    \n",
    "    full_pipeline = Pipeline([\n",
    "        #(\"categorical_encoder\", CategoricalEncoder()),\n",
    "        (\"model\",xgb.XGBRegressor(enable_categorical=True, **xgboost_params, device=\"cuda\", tree_method=\"hist\"))\n",
    "    ])\n",
    "    full_pipeline.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_train_fold)), np.expm1(y_train_fold))\n",
    "    val_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_val_fold)), np.expm1(y_val_fold))\n",
    "\n",
    "    print(f\"Train: {train_score}, Test: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'boosting_type': 'gbdt', 'objective': 'regression', 'learning_rate': 0.0239614211663401, 'num_boost_round': 826, 'num_leaves': 169, 'max_depth': 34, 'feature_fraction': 0.5772473473750267, 'bagging_fraction': 0.5660377739630706, 'bagging_freq': 2, 'min_child_samples': 32, 'min_split_gain': 0.04367672923191071, 'min_child_weight': 3.793554610841873, 'lambda_l1': 4.429544342866135, 'lambda_l2': 2.0212763479020808, 'cat_smooth': 26.99333583992606, 'max_bin': 168, 'verbose': -1, 'random_state': 41}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9910957591564978, Test: 1.0347058373067353\n",
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9913558298300467, Test: 1.0336348923577352\n",
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9906868469726866, Test: 1.0352591155530728\n",
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9913053658783485, Test: 1.033069399644675\n",
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9909832782721372, Test: 1.0347993281192245\n"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in splitter.split(X_train):\n",
    "    print(\"Starting\")\n",
    "    # Prepare data for LightGBM\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    y_train_fold = np.log1p(y_train_fold)\n",
    "    y_val_fold = np.log1p(y_val_fold)\n",
    "    \n",
    "    full_pipeline = Pipeline([\n",
    "        (\"model\",LGBMRegressor(**lgbm_params))\n",
    "    ])\n",
    "    full_pipeline.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    \n",
    "    train_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_train_fold)), np.expm1(y_train_fold))\n",
    "    val_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_val_fold)), np.expm1(y_val_fold))\n",
    "\n",
    "    print(f\"Train: {train_score}, Test: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {'learning_rate': 0.30849158244687513, 'iterations': 74, 'depth': 7, 'l2_leaf_reg': 0.9408913103331975, 'min_data_in_leaf': 43, 'subsample': 0.6707321357155331, 'colsample_bylevel': 0.8996427501620363}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "0:\tlearn: 1.0696023\ttotal: 223ms\tremaining: 16.3s\n",
      "1:\tlearn: 1.0559500\ttotal: 389ms\tremaining: 14s\n",
      "2:\tlearn: 1.0483943\ttotal: 554ms\tremaining: 13.1s\n",
      "3:\tlearn: 1.0444098\ttotal: 705ms\tremaining: 12.3s\n",
      "4:\tlearn: 1.0413685\ttotal: 887ms\tremaining: 12.2s\n",
      "5:\tlearn: 1.0396677\ttotal: 1.04s\tremaining: 11.8s\n",
      "6:\tlearn: 1.0383621\ttotal: 1.19s\tremaining: 11.4s\n",
      "7:\tlearn: 1.0376821\ttotal: 1.33s\tremaining: 11s\n",
      "8:\tlearn: 1.0372948\ttotal: 1.5s\tremaining: 10.9s\n",
      "9:\tlearn: 1.0366547\ttotal: 1.66s\tremaining: 10.6s\n",
      "10:\tlearn: 1.0362325\ttotal: 1.81s\tremaining: 10.3s\n",
      "11:\tlearn: 1.0359433\ttotal: 1.95s\tremaining: 10.1s\n",
      "12:\tlearn: 1.0357044\ttotal: 2.08s\tremaining: 9.76s\n",
      "13:\tlearn: 1.0355004\ttotal: 2.21s\tremaining: 9.48s\n",
      "14:\tlearn: 1.0353417\ttotal: 2.36s\tremaining: 9.27s\n",
      "15:\tlearn: 1.0352423\ttotal: 2.48s\tremaining: 8.99s\n",
      "16:\tlearn: 1.0349886\ttotal: 2.62s\tremaining: 8.78s\n",
      "17:\tlearn: 1.0349217\ttotal: 2.75s\tremaining: 8.55s\n",
      "18:\tlearn: 1.0346380\ttotal: 2.88s\tremaining: 8.34s\n",
      "19:\tlearn: 1.0344495\ttotal: 3.01s\tremaining: 8.12s\n",
      "20:\tlearn: 1.0343762\ttotal: 3.14s\tremaining: 7.92s\n",
      "21:\tlearn: 1.0343127\ttotal: 3.29s\tremaining: 7.77s\n",
      "22:\tlearn: 1.0340733\ttotal: 3.43s\tremaining: 7.61s\n",
      "23:\tlearn: 1.0340045\ttotal: 3.56s\tremaining: 7.41s\n",
      "24:\tlearn: 1.0338678\ttotal: 3.69s\tremaining: 7.23s\n",
      "25:\tlearn: 1.0336849\ttotal: 3.83s\tremaining: 7.06s\n",
      "26:\tlearn: 1.0335664\ttotal: 3.97s\tremaining: 6.92s\n",
      "27:\tlearn: 1.0334918\ttotal: 4.11s\tremaining: 6.75s\n",
      "28:\tlearn: 1.0334726\ttotal: 4.26s\tremaining: 6.61s\n",
      "29:\tlearn: 1.0334447\ttotal: 4.41s\tremaining: 6.47s\n",
      "30:\tlearn: 1.0333641\ttotal: 4.54s\tremaining: 6.29s\n",
      "31:\tlearn: 1.0333507\ttotal: 4.7s\tremaining: 6.16s\n",
      "32:\tlearn: 1.0332916\ttotal: 4.84s\tremaining: 6.01s\n",
      "33:\tlearn: 1.0332623\ttotal: 4.99s\tremaining: 5.87s\n",
      "34:\tlearn: 1.0332107\ttotal: 5.12s\tremaining: 5.7s\n",
      "35:\tlearn: 1.0332090\ttotal: 5.18s\tremaining: 5.47s\n",
      "36:\tlearn: 1.0331010\ttotal: 5.32s\tremaining: 5.32s\n",
      "37:\tlearn: 1.0330383\ttotal: 5.47s\tremaining: 5.18s\n",
      "38:\tlearn: 1.0329773\ttotal: 5.61s\tremaining: 5.03s\n",
      "39:\tlearn: 1.0328670\ttotal: 5.74s\tremaining: 4.88s\n",
      "40:\tlearn: 1.0328154\ttotal: 5.88s\tremaining: 4.73s\n",
      "41:\tlearn: 1.0327860\ttotal: 6.04s\tremaining: 4.6s\n",
      "42:\tlearn: 1.0327441\ttotal: 6.18s\tremaining: 4.46s\n",
      "43:\tlearn: 1.0327135\ttotal: 6.32s\tremaining: 4.31s\n",
      "44:\tlearn: 1.0326862\ttotal: 6.46s\tremaining: 4.16s\n",
      "45:\tlearn: 1.0326661\ttotal: 6.6s\tremaining: 4.02s\n",
      "46:\tlearn: 1.0325021\ttotal: 6.74s\tremaining: 3.87s\n",
      "47:\tlearn: 1.0324504\ttotal: 6.89s\tremaining: 3.73s\n",
      "48:\tlearn: 1.0323906\ttotal: 7.02s\tremaining: 3.58s\n",
      "49:\tlearn: 1.0323188\ttotal: 7.17s\tremaining: 3.44s\n",
      "50:\tlearn: 1.0322830\ttotal: 7.33s\tremaining: 3.31s\n",
      "51:\tlearn: 1.0322620\ttotal: 7.49s\tremaining: 3.17s\n",
      "52:\tlearn: 1.0322003\ttotal: 7.62s\tremaining: 3.02s\n",
      "53:\tlearn: 1.0320606\ttotal: 7.76s\tremaining: 2.87s\n",
      "54:\tlearn: 1.0319912\ttotal: 7.91s\tremaining: 2.73s\n",
      "55:\tlearn: 1.0319212\ttotal: 8.05s\tremaining: 2.59s\n",
      "56:\tlearn: 1.0318704\ttotal: 8.2s\tremaining: 2.44s\n",
      "57:\tlearn: 1.0318149\ttotal: 8.33s\tremaining: 2.3s\n",
      "58:\tlearn: 1.0317618\ttotal: 8.47s\tremaining: 2.15s\n",
      "59:\tlearn: 1.0317599\ttotal: 8.6s\tremaining: 2.01s\n",
      "60:\tlearn: 1.0316625\ttotal: 8.75s\tremaining: 1.86s\n",
      "61:\tlearn: 1.0316204\ttotal: 8.91s\tremaining: 1.72s\n",
      "62:\tlearn: 1.0315585\ttotal: 9.06s\tremaining: 1.58s\n",
      "63:\tlearn: 1.0315114\ttotal: 9.21s\tremaining: 1.44s\n",
      "64:\tlearn: 1.0314646\ttotal: 9.36s\tremaining: 1.29s\n",
      "65:\tlearn: 1.0314401\ttotal: 9.5s\tremaining: 1.15s\n",
      "66:\tlearn: 1.0313559\ttotal: 9.64s\tremaining: 1.01s\n",
      "67:\tlearn: 1.0313216\ttotal: 9.79s\tremaining: 863ms\n",
      "68:\tlearn: 1.0312898\ttotal: 9.93s\tremaining: 720ms\n",
      "69:\tlearn: 1.0312125\ttotal: 10.1s\tremaining: 575ms\n",
      "70:\tlearn: 1.0311835\ttotal: 10.2s\tremaining: 431ms\n",
      "71:\tlearn: 1.0310928\ttotal: 10.3s\tremaining: 287ms\n",
      "72:\tlearn: 1.0310182\ttotal: 10.5s\tremaining: 144ms\n",
      "73:\tlearn: 1.0309692\ttotal: 10.6s\tremaining: 0us\n",
      "Train: 1.03106683857008, Test: 1.0340327592706833\n",
      "Starting\n",
      "0:\tlearn: 1.0697100\ttotal: 192ms\tremaining: 14s\n",
      "1:\tlearn: 1.0563234\ttotal: 370ms\tremaining: 13.3s\n",
      "2:\tlearn: 1.0485210\ttotal: 526ms\tremaining: 12.5s\n",
      "3:\tlearn: 1.0445784\ttotal: 698ms\tremaining: 12.2s\n",
      "4:\tlearn: 1.0421934\ttotal: 832ms\tremaining: 11.5s\n",
      "5:\tlearn: 1.0405366\ttotal: 1.01s\tremaining: 11.4s\n",
      "6:\tlearn: 1.0394627\ttotal: 1.14s\tremaining: 10.9s\n",
      "7:\tlearn: 1.0385672\ttotal: 1.28s\tremaining: 10.6s\n",
      "8:\tlearn: 1.0381500\ttotal: 1.44s\tremaining: 10.4s\n",
      "9:\tlearn: 1.0375219\ttotal: 1.59s\tremaining: 10.2s\n",
      "10:\tlearn: 1.0371576\ttotal: 1.72s\tremaining: 9.84s\n",
      "11:\tlearn: 1.0367319\ttotal: 1.84s\tremaining: 9.53s\n",
      "12:\tlearn: 1.0364518\ttotal: 1.98s\tremaining: 9.27s\n",
      "13:\tlearn: 1.0361854\ttotal: 2.12s\tremaining: 9.07s\n",
      "14:\tlearn: 1.0359522\ttotal: 2.24s\tremaining: 8.83s\n",
      "15:\tlearn: 1.0358476\ttotal: 2.36s\tremaining: 8.57s\n",
      "16:\tlearn: 1.0357668\ttotal: 2.49s\tremaining: 8.36s\n",
      "17:\tlearn: 1.0356184\ttotal: 2.63s\tremaining: 8.19s\n",
      "18:\tlearn: 1.0353020\ttotal: 2.77s\tremaining: 8.02s\n",
      "19:\tlearn: 1.0352141\ttotal: 2.9s\tremaining: 7.84s\n",
      "20:\tlearn: 1.0351417\ttotal: 3.04s\tremaining: 7.67s\n",
      "21:\tlearn: 1.0349471\ttotal: 3.18s\tremaining: 7.51s\n",
      "22:\tlearn: 1.0348982\ttotal: 3.31s\tremaining: 7.34s\n",
      "23:\tlearn: 1.0348916\ttotal: 3.46s\tremaining: 7.2s\n",
      "24:\tlearn: 1.0346248\ttotal: 3.58s\tremaining: 7.02s\n",
      "25:\tlearn: 1.0346034\ttotal: 3.74s\tremaining: 6.91s\n",
      "26:\tlearn: 1.0345165\ttotal: 3.87s\tremaining: 6.73s\n",
      "27:\tlearn: 1.0342745\ttotal: 4s\tremaining: 6.57s\n",
      "28:\tlearn: 1.0342606\ttotal: 4.14s\tremaining: 6.43s\n",
      "29:\tlearn: 1.0342457\ttotal: 4.29s\tremaining: 6.3s\n",
      "30:\tlearn: 1.0341427\ttotal: 4.43s\tremaining: 6.14s\n",
      "31:\tlearn: 1.0341261\ttotal: 4.56s\tremaining: 5.99s\n",
      "32:\tlearn: 1.0341242\ttotal: 4.68s\tremaining: 5.82s\n",
      "33:\tlearn: 1.0340933\ttotal: 4.82s\tremaining: 5.67s\n",
      "34:\tlearn: 1.0340596\ttotal: 4.96s\tremaining: 5.53s\n",
      "35:\tlearn: 1.0340114\ttotal: 5.08s\tremaining: 5.37s\n",
      "36:\tlearn: 1.0339874\ttotal: 5.22s\tremaining: 5.22s\n",
      "37:\tlearn: 1.0339419\ttotal: 5.36s\tremaining: 5.08s\n",
      "38:\tlearn: 1.0339053\ttotal: 5.51s\tremaining: 4.95s\n",
      "39:\tlearn: 1.0338155\ttotal: 5.64s\tremaining: 4.79s\n",
      "40:\tlearn: 1.0337177\ttotal: 5.77s\tremaining: 4.65s\n",
      "41:\tlearn: 1.0337142\ttotal: 5.9s\tremaining: 4.5s\n",
      "42:\tlearn: 1.0335699\ttotal: 6.05s\tremaining: 4.36s\n",
      "43:\tlearn: 1.0335008\ttotal: 6.2s\tremaining: 4.23s\n",
      "44:\tlearn: 1.0334733\ttotal: 6.34s\tremaining: 4.09s\n",
      "45:\tlearn: 1.0333235\ttotal: 6.48s\tremaining: 3.94s\n",
      "46:\tlearn: 1.0332770\ttotal: 6.63s\tremaining: 3.81s\n",
      "47:\tlearn: 1.0332670\ttotal: 6.77s\tremaining: 3.67s\n",
      "48:\tlearn: 1.0332419\ttotal: 6.91s\tremaining: 3.52s\n",
      "49:\tlearn: 1.0331483\ttotal: 7.04s\tremaining: 3.38s\n",
      "50:\tlearn: 1.0330658\ttotal: 7.19s\tremaining: 3.24s\n",
      "51:\tlearn: 1.0329118\ttotal: 7.33s\tremaining: 3.1s\n",
      "52:\tlearn: 1.0328194\ttotal: 7.47s\tremaining: 2.96s\n",
      "53:\tlearn: 1.0327447\ttotal: 7.6s\tremaining: 2.81s\n",
      "54:\tlearn: 1.0326967\ttotal: 7.74s\tremaining: 2.67s\n",
      "55:\tlearn: 1.0326350\ttotal: 7.86s\tremaining: 2.53s\n",
      "56:\tlearn: 1.0325648\ttotal: 7.99s\tremaining: 2.38s\n",
      "57:\tlearn: 1.0325167\ttotal: 8.13s\tremaining: 2.24s\n",
      "58:\tlearn: 1.0324608\ttotal: 8.26s\tremaining: 2.1s\n",
      "59:\tlearn: 1.0324152\ttotal: 8.39s\tremaining: 1.96s\n",
      "60:\tlearn: 1.0323342\ttotal: 8.53s\tremaining: 1.82s\n",
      "61:\tlearn: 1.0322616\ttotal: 8.67s\tremaining: 1.68s\n",
      "62:\tlearn: 1.0321660\ttotal: 8.79s\tremaining: 1.53s\n",
      "63:\tlearn: 1.0321564\ttotal: 8.93s\tremaining: 1.4s\n",
      "64:\tlearn: 1.0321018\ttotal: 9.07s\tremaining: 1.26s\n",
      "65:\tlearn: 1.0320453\ttotal: 9.22s\tremaining: 1.12s\n",
      "66:\tlearn: 1.0320154\ttotal: 9.35s\tremaining: 977ms\n",
      "67:\tlearn: 1.0319532\ttotal: 9.49s\tremaining: 838ms\n",
      "68:\tlearn: 1.0318942\ttotal: 9.65s\tremaining: 699ms\n",
      "69:\tlearn: 1.0318494\ttotal: 9.79s\tremaining: 559ms\n",
      "70:\tlearn: 1.0317747\ttotal: 9.93s\tremaining: 420ms\n",
      "71:\tlearn: 1.0317377\ttotal: 10.1s\tremaining: 280ms\n",
      "72:\tlearn: 1.0316565\ttotal: 10.2s\tremaining: 140ms\n",
      "73:\tlearn: 1.0316075\ttotal: 10.3s\tremaining: 0us\n",
      "Train: 1.0316835889167553, Test: 1.0333164673633704\n",
      "Starting\n",
      "0:\tlearn: 1.0698643\ttotal: 184ms\tremaining: 13.4s\n",
      "1:\tlearn: 1.0558022\ttotal: 346ms\tremaining: 12.5s\n",
      "2:\tlearn: 1.0487918\ttotal: 505ms\tremaining: 11.9s\n",
      "3:\tlearn: 1.0444112\ttotal: 695ms\tremaining: 12.2s\n",
      "4:\tlearn: 1.0418447\ttotal: 859ms\tremaining: 11.9s\n",
      "5:\tlearn: 1.0399809\ttotal: 1s\tremaining: 11.4s\n",
      "6:\tlearn: 1.0387627\ttotal: 1.14s\tremaining: 10.9s\n",
      "7:\tlearn: 1.0377279\ttotal: 1.28s\tremaining: 10.5s\n",
      "8:\tlearn: 1.0372380\ttotal: 1.43s\tremaining: 10.4s\n",
      "9:\tlearn: 1.0368252\ttotal: 1.58s\tremaining: 10.1s\n",
      "10:\tlearn: 1.0365594\ttotal: 1.71s\tremaining: 9.81s\n",
      "11:\tlearn: 1.0365148\ttotal: 1.78s\tremaining: 9.22s\n",
      "12:\tlearn: 1.0360308\ttotal: 1.93s\tremaining: 9.05s\n",
      "13:\tlearn: 1.0357754\ttotal: 2.06s\tremaining: 8.85s\n",
      "14:\tlearn: 1.0355274\ttotal: 2.2s\tremaining: 8.64s\n",
      "15:\tlearn: 1.0352912\ttotal: 2.33s\tremaining: 8.43s\n",
      "16:\tlearn: 1.0349624\ttotal: 2.46s\tremaining: 8.26s\n",
      "17:\tlearn: 1.0348378\ttotal: 2.59s\tremaining: 8.05s\n",
      "18:\tlearn: 1.0347463\ttotal: 2.72s\tremaining: 7.87s\n",
      "19:\tlearn: 1.0346116\ttotal: 2.86s\tremaining: 7.73s\n",
      "20:\tlearn: 1.0344540\ttotal: 2.98s\tremaining: 7.53s\n",
      "21:\tlearn: 1.0344401\ttotal: 3.13s\tremaining: 7.4s\n",
      "22:\tlearn: 1.0343116\ttotal: 3.25s\tremaining: 7.21s\n",
      "23:\tlearn: 1.0342640\ttotal: 3.39s\tremaining: 7.07s\n",
      "24:\tlearn: 1.0341797\ttotal: 3.53s\tremaining: 6.91s\n",
      "25:\tlearn: 1.0340433\ttotal: 3.66s\tremaining: 6.76s\n",
      "26:\tlearn: 1.0339819\ttotal: 3.81s\tremaining: 6.63s\n",
      "27:\tlearn: 1.0338757\ttotal: 3.94s\tremaining: 6.48s\n",
      "28:\tlearn: 1.0338032\ttotal: 4.07s\tremaining: 6.32s\n",
      "29:\tlearn: 1.0338030\ttotal: 4.13s\tremaining: 6.07s\n",
      "30:\tlearn: 1.0337555\ttotal: 4.3s\tremaining: 5.97s\n",
      "31:\tlearn: 1.0337031\ttotal: 4.44s\tremaining: 5.82s\n",
      "32:\tlearn: 1.0337020\ttotal: 4.49s\tremaining: 5.58s\n",
      "33:\tlearn: 1.0336396\ttotal: 4.61s\tremaining: 5.42s\n",
      "34:\tlearn: 1.0335889\ttotal: 4.75s\tremaining: 5.29s\n",
      "35:\tlearn: 1.0335865\ttotal: 4.88s\tremaining: 5.16s\n",
      "36:\tlearn: 1.0335817\ttotal: 4.99s\tremaining: 4.99s\n",
      "37:\tlearn: 1.0335137\ttotal: 5.13s\tremaining: 4.86s\n",
      "38:\tlearn: 1.0334952\ttotal: 5.28s\tremaining: 4.74s\n",
      "39:\tlearn: 1.0334813\ttotal: 5.41s\tremaining: 4.6s\n",
      "40:\tlearn: 1.0334177\ttotal: 5.54s\tremaining: 4.46s\n",
      "41:\tlearn: 1.0333579\ttotal: 5.67s\tremaining: 4.32s\n",
      "42:\tlearn: 1.0332446\ttotal: 5.81s\tremaining: 4.19s\n",
      "43:\tlearn: 1.0332043\ttotal: 5.95s\tremaining: 4.05s\n",
      "44:\tlearn: 1.0331835\ttotal: 6.12s\tremaining: 3.94s\n",
      "45:\tlearn: 1.0330946\ttotal: 6.29s\tremaining: 3.83s\n",
      "46:\tlearn: 1.0329046\ttotal: 6.46s\tremaining: 3.71s\n",
      "47:\tlearn: 1.0327951\ttotal: 6.61s\tremaining: 3.58s\n",
      "48:\tlearn: 1.0327662\ttotal: 6.76s\tremaining: 3.45s\n",
      "49:\tlearn: 1.0326645\ttotal: 6.91s\tremaining: 3.31s\n",
      "50:\tlearn: 1.0326120\ttotal: 7.04s\tremaining: 3.18s\n",
      "51:\tlearn: 1.0325199\ttotal: 7.17s\tremaining: 3.03s\n",
      "52:\tlearn: 1.0324548\ttotal: 7.31s\tremaining: 2.9s\n",
      "53:\tlearn: 1.0323804\ttotal: 7.44s\tremaining: 2.75s\n",
      "54:\tlearn: 1.0321827\ttotal: 7.58s\tremaining: 2.62s\n",
      "55:\tlearn: 1.0320866\ttotal: 7.74s\tremaining: 2.49s\n",
      "56:\tlearn: 1.0320633\ttotal: 7.87s\tremaining: 2.35s\n",
      "57:\tlearn: 1.0319515\ttotal: 8.01s\tremaining: 2.21s\n",
      "58:\tlearn: 1.0318995\ttotal: 8.15s\tremaining: 2.07s\n",
      "59:\tlearn: 1.0318212\ttotal: 8.29s\tremaining: 1.93s\n",
      "60:\tlearn: 1.0317743\ttotal: 8.43s\tremaining: 1.79s\n",
      "61:\tlearn: 1.0317055\ttotal: 8.57s\tremaining: 1.66s\n",
      "62:\tlearn: 1.0316592\ttotal: 8.7s\tremaining: 1.52s\n",
      "63:\tlearn: 1.0316327\ttotal: 8.84s\tremaining: 1.38s\n",
      "64:\tlearn: 1.0316149\ttotal: 8.98s\tremaining: 1.24s\n",
      "65:\tlearn: 1.0315829\ttotal: 9.11s\tremaining: 1.1s\n",
      "66:\tlearn: 1.0314938\ttotal: 9.24s\tremaining: 966ms\n",
      "67:\tlearn: 1.0314553\ttotal: 9.38s\tremaining: 828ms\n",
      "68:\tlearn: 1.0314376\ttotal: 9.51s\tremaining: 689ms\n",
      "69:\tlearn: 1.0313768\ttotal: 9.66s\tremaining: 552ms\n",
      "70:\tlearn: 1.0312882\ttotal: 9.79s\tremaining: 414ms\n",
      "71:\tlearn: 1.0312692\ttotal: 9.93s\tremaining: 276ms\n",
      "72:\tlearn: 1.0312319\ttotal: 10.1s\tremaining: 138ms\n",
      "73:\tlearn: 1.0311824\ttotal: 10.2s\tremaining: 0us\n",
      "Train: 1.0313027154682144, Test: 1.0355132789441632\n",
      "Starting\n",
      "0:\tlearn: 1.0697430\ttotal: 170ms\tremaining: 12.4s\n",
      "1:\tlearn: 1.0564339\ttotal: 324ms\tremaining: 11.7s\n",
      "2:\tlearn: 1.0492466\ttotal: 469ms\tremaining: 11.1s\n",
      "3:\tlearn: 1.0453542\ttotal: 619ms\tremaining: 10.8s\n",
      "4:\tlearn: 1.0425649\ttotal: 777ms\tremaining: 10.7s\n",
      "5:\tlearn: 1.0408322\ttotal: 957ms\tremaining: 10.8s\n",
      "6:\tlearn: 1.0398341\ttotal: 1.1s\tremaining: 10.5s\n",
      "7:\tlearn: 1.0387749\ttotal: 1.24s\tremaining: 10.2s\n",
      "8:\tlearn: 1.0383787\ttotal: 1.38s\tremaining: 9.94s\n",
      "9:\tlearn: 1.0379091\ttotal: 1.52s\tremaining: 9.75s\n",
      "10:\tlearn: 1.0375664\ttotal: 1.67s\tremaining: 9.54s\n",
      "11:\tlearn: 1.0372351\ttotal: 1.81s\tremaining: 9.33s\n",
      "12:\tlearn: 1.0370763\ttotal: 1.95s\tremaining: 9.13s\n",
      "13:\tlearn: 1.0367231\ttotal: 2.07s\tremaining: 8.87s\n",
      "14:\tlearn: 1.0363890\ttotal: 2.2s\tremaining: 8.65s\n",
      "15:\tlearn: 1.0361549\ttotal: 2.33s\tremaining: 8.46s\n",
      "16:\tlearn: 1.0359524\ttotal: 2.48s\tremaining: 8.33s\n",
      "17:\tlearn: 1.0358745\ttotal: 2.61s\tremaining: 8.11s\n",
      "18:\tlearn: 1.0357009\ttotal: 2.75s\tremaining: 7.97s\n",
      "19:\tlearn: 1.0356948\ttotal: 2.81s\tremaining: 7.58s\n",
      "20:\tlearn: 1.0356654\ttotal: 2.94s\tremaining: 7.43s\n",
      "21:\tlearn: 1.0356422\ttotal: 3.09s\tremaining: 7.31s\n",
      "22:\tlearn: 1.0354197\ttotal: 3.23s\tremaining: 7.16s\n",
      "23:\tlearn: 1.0353239\ttotal: 3.37s\tremaining: 7.02s\n",
      "24:\tlearn: 1.0350678\ttotal: 3.5s\tremaining: 6.86s\n",
      "25:\tlearn: 1.0350359\ttotal: 3.64s\tremaining: 6.73s\n",
      "26:\tlearn: 1.0349697\ttotal: 3.78s\tremaining: 6.58s\n",
      "27:\tlearn: 1.0348825\ttotal: 3.9s\tremaining: 6.41s\n",
      "28:\tlearn: 1.0346750\ttotal: 4.05s\tremaining: 6.28s\n",
      "29:\tlearn: 1.0346192\ttotal: 4.18s\tremaining: 6.14s\n",
      "30:\tlearn: 1.0345130\ttotal: 4.32s\tremaining: 5.99s\n",
      "31:\tlearn: 1.0344817\ttotal: 4.45s\tremaining: 5.84s\n",
      "32:\tlearn: 1.0344307\ttotal: 4.59s\tremaining: 5.7s\n",
      "33:\tlearn: 1.0344114\ttotal: 4.73s\tremaining: 5.57s\n",
      "34:\tlearn: 1.0343918\ttotal: 4.86s\tremaining: 5.42s\n",
      "35:\tlearn: 1.0343165\ttotal: 5.01s\tremaining: 5.29s\n",
      "36:\tlearn: 1.0341741\ttotal: 5.14s\tremaining: 5.14s\n",
      "37:\tlearn: 1.0341544\ttotal: 5.28s\tremaining: 5s\n",
      "38:\tlearn: 1.0341354\ttotal: 5.42s\tremaining: 4.86s\n",
      "39:\tlearn: 1.0340918\ttotal: 5.56s\tremaining: 4.73s\n",
      "40:\tlearn: 1.0340302\ttotal: 5.69s\tremaining: 4.58s\n",
      "41:\tlearn: 1.0340121\ttotal: 5.84s\tremaining: 4.45s\n",
      "42:\tlearn: 1.0338752\ttotal: 5.98s\tremaining: 4.31s\n",
      "43:\tlearn: 1.0338523\ttotal: 6.12s\tremaining: 4.17s\n",
      "44:\tlearn: 1.0337703\ttotal: 6.25s\tremaining: 4.03s\n",
      "45:\tlearn: 1.0336914\ttotal: 6.37s\tremaining: 3.88s\n",
      "46:\tlearn: 1.0335244\ttotal: 6.51s\tremaining: 3.74s\n",
      "47:\tlearn: 1.0334932\ttotal: 6.65s\tremaining: 3.6s\n",
      "48:\tlearn: 1.0334337\ttotal: 6.78s\tremaining: 3.46s\n",
      "49:\tlearn: 1.0333847\ttotal: 6.92s\tremaining: 3.32s\n",
      "50:\tlearn: 1.0333788\ttotal: 7.05s\tremaining: 3.18s\n",
      "51:\tlearn: 1.0332418\ttotal: 7.19s\tremaining: 3.04s\n",
      "52:\tlearn: 1.0331576\ttotal: 7.33s\tremaining: 2.91s\n",
      "53:\tlearn: 1.0330960\ttotal: 7.47s\tremaining: 2.77s\n",
      "54:\tlearn: 1.0330381\ttotal: 7.6s\tremaining: 2.62s\n",
      "55:\tlearn: 1.0329617\ttotal: 7.73s\tremaining: 2.48s\n",
      "56:\tlearn: 1.0329080\ttotal: 7.87s\tremaining: 2.35s\n",
      "57:\tlearn: 1.0328651\ttotal: 8.01s\tremaining: 2.21s\n",
      "58:\tlearn: 1.0328326\ttotal: 8.15s\tremaining: 2.07s\n",
      "59:\tlearn: 1.0327039\ttotal: 8.3s\tremaining: 1.94s\n",
      "60:\tlearn: 1.0326615\ttotal: 8.44s\tremaining: 1.8s\n",
      "61:\tlearn: 1.0326334\ttotal: 8.58s\tremaining: 1.66s\n",
      "62:\tlearn: 1.0325298\ttotal: 8.71s\tremaining: 1.52s\n",
      "63:\tlearn: 1.0324957\ttotal: 8.87s\tremaining: 1.39s\n",
      "64:\tlearn: 1.0324502\ttotal: 9s\tremaining: 1.25s\n",
      "65:\tlearn: 1.0323654\ttotal: 9.13s\tremaining: 1.11s\n",
      "66:\tlearn: 1.0322959\ttotal: 9.26s\tremaining: 968ms\n",
      "67:\tlearn: 1.0322477\ttotal: 9.41s\tremaining: 830ms\n",
      "68:\tlearn: 1.0322192\ttotal: 9.55s\tremaining: 692ms\n",
      "69:\tlearn: 1.0321635\ttotal: 9.69s\tremaining: 554ms\n",
      "70:\tlearn: 1.0321521\ttotal: 9.83s\tremaining: 416ms\n",
      "71:\tlearn: 1.0320755\ttotal: 9.96s\tremaining: 277ms\n",
      "72:\tlearn: 1.0320339\ttotal: 10.1s\tremaining: 138ms\n",
      "73:\tlearn: 1.0319616\ttotal: 10.2s\tremaining: 0us\n",
      "Train: 1.0320654183021833, Test: 1.0320322464723293\n",
      "Starting\n",
      "0:\tlearn: 1.0695795\ttotal: 200ms\tremaining: 14.6s\n",
      "1:\tlearn: 1.0560512\ttotal: 355ms\tremaining: 12.8s\n",
      "2:\tlearn: 1.0484129\ttotal: 508ms\tremaining: 12s\n",
      "3:\tlearn: 1.0444154\ttotal: 658ms\tremaining: 11.5s\n",
      "4:\tlearn: 1.0415748\ttotal: 847ms\tremaining: 11.7s\n",
      "5:\tlearn: 1.0399347\ttotal: 997ms\tremaining: 11.3s\n",
      "6:\tlearn: 1.0386422\ttotal: 1.15s\tremaining: 11s\n",
      "7:\tlearn: 1.0380141\ttotal: 1.29s\tremaining: 10.7s\n",
      "8:\tlearn: 1.0373961\ttotal: 1.47s\tremaining: 10.6s\n",
      "9:\tlearn: 1.0370742\ttotal: 1.62s\tremaining: 10.4s\n",
      "10:\tlearn: 1.0366710\ttotal: 1.75s\tremaining: 10s\n",
      "11:\tlearn: 1.0364654\ttotal: 1.89s\tremaining: 9.75s\n",
      "12:\tlearn: 1.0363220\ttotal: 2.02s\tremaining: 9.47s\n",
      "13:\tlearn: 1.0361279\ttotal: 2.14s\tremaining: 9.19s\n",
      "14:\tlearn: 1.0358571\ttotal: 2.28s\tremaining: 8.96s\n",
      "15:\tlearn: 1.0356016\ttotal: 2.41s\tremaining: 8.75s\n",
      "16:\tlearn: 1.0353769\ttotal: 2.54s\tremaining: 8.53s\n",
      "17:\tlearn: 1.0352214\ttotal: 2.7s\tremaining: 8.4s\n",
      "18:\tlearn: 1.0349360\ttotal: 2.87s\tremaining: 8.29s\n",
      "19:\tlearn: 1.0347210\ttotal: 2.98s\tremaining: 8.05s\n",
      "20:\tlearn: 1.0347193\ttotal: 3.03s\tremaining: 7.65s\n",
      "21:\tlearn: 1.0346713\ttotal: 3.18s\tremaining: 7.51s\n",
      "22:\tlearn: 1.0346022\ttotal: 3.31s\tremaining: 7.33s\n",
      "23:\tlearn: 1.0344175\ttotal: 3.44s\tremaining: 7.17s\n",
      "24:\tlearn: 1.0343159\ttotal: 3.58s\tremaining: 7.01s\n",
      "25:\tlearn: 1.0342537\ttotal: 3.74s\tremaining: 6.9s\n",
      "26:\tlearn: 1.0342393\ttotal: 3.89s\tremaining: 6.77s\n",
      "27:\tlearn: 1.0341591\ttotal: 4.02s\tremaining: 6.61s\n",
      "28:\tlearn: 1.0340969\ttotal: 4.17s\tremaining: 6.47s\n",
      "29:\tlearn: 1.0340344\ttotal: 4.3s\tremaining: 6.31s\n",
      "30:\tlearn: 1.0340302\ttotal: 4.39s\tremaining: 6.09s\n",
      "31:\tlearn: 1.0340215\ttotal: 4.53s\tremaining: 5.94s\n",
      "32:\tlearn: 1.0340056\ttotal: 4.68s\tremaining: 5.82s\n",
      "33:\tlearn: 1.0339636\ttotal: 4.84s\tremaining: 5.69s\n",
      "34:\tlearn: 1.0338780\ttotal: 4.97s\tremaining: 5.54s\n",
      "35:\tlearn: 1.0337831\ttotal: 5.1s\tremaining: 5.39s\n",
      "36:\tlearn: 1.0335958\ttotal: 5.24s\tremaining: 5.24s\n",
      "37:\tlearn: 1.0335291\ttotal: 5.38s\tremaining: 5.1s\n",
      "38:\tlearn: 1.0334992\ttotal: 5.52s\tremaining: 4.95s\n",
      "39:\tlearn: 1.0334799\ttotal: 5.66s\tremaining: 4.81s\n",
      "40:\tlearn: 1.0334323\ttotal: 5.79s\tremaining: 4.66s\n",
      "41:\tlearn: 1.0333943\ttotal: 5.93s\tremaining: 4.52s\n",
      "42:\tlearn: 1.0333334\ttotal: 6.07s\tremaining: 4.37s\n",
      "43:\tlearn: 1.0331822\ttotal: 6.2s\tremaining: 4.23s\n",
      "44:\tlearn: 1.0331305\ttotal: 6.34s\tremaining: 4.09s\n",
      "45:\tlearn: 1.0330814\ttotal: 6.48s\tremaining: 3.95s\n",
      "46:\tlearn: 1.0330527\ttotal: 6.62s\tremaining: 3.8s\n",
      "47:\tlearn: 1.0329846\ttotal: 6.75s\tremaining: 3.66s\n",
      "48:\tlearn: 1.0329657\ttotal: 6.89s\tremaining: 3.52s\n",
      "49:\tlearn: 1.0328757\ttotal: 7.03s\tremaining: 3.37s\n",
      "50:\tlearn: 1.0328086\ttotal: 7.16s\tremaining: 3.23s\n",
      "51:\tlearn: 1.0327155\ttotal: 7.28s\tremaining: 3.08s\n",
      "52:\tlearn: 1.0325928\ttotal: 7.43s\tremaining: 2.94s\n",
      "53:\tlearn: 1.0325564\ttotal: 7.57s\tremaining: 2.8s\n",
      "54:\tlearn: 1.0324065\ttotal: 7.71s\tremaining: 2.66s\n",
      "55:\tlearn: 1.0323186\ttotal: 7.85s\tremaining: 2.52s\n",
      "56:\tlearn: 1.0322385\ttotal: 8s\tremaining: 2.38s\n",
      "57:\tlearn: 1.0321531\ttotal: 8.12s\tremaining: 2.24s\n",
      "58:\tlearn: 1.0321032\ttotal: 8.26s\tremaining: 2.1s\n",
      "59:\tlearn: 1.0320316\ttotal: 8.39s\tremaining: 1.96s\n",
      "60:\tlearn: 1.0319501\ttotal: 8.54s\tremaining: 1.82s\n",
      "61:\tlearn: 1.0318901\ttotal: 8.7s\tremaining: 1.68s\n",
      "62:\tlearn: 1.0318491\ttotal: 8.84s\tremaining: 1.54s\n",
      "63:\tlearn: 1.0317834\ttotal: 8.97s\tremaining: 1.4s\n",
      "64:\tlearn: 1.0316868\ttotal: 9.11s\tremaining: 1.26s\n",
      "65:\tlearn: 1.0316356\ttotal: 9.25s\tremaining: 1.12s\n",
      "66:\tlearn: 1.0315657\ttotal: 9.38s\tremaining: 980ms\n",
      "67:\tlearn: 1.0314878\ttotal: 9.52s\tremaining: 840ms\n",
      "68:\tlearn: 1.0314046\ttotal: 9.65s\tremaining: 699ms\n",
      "69:\tlearn: 1.0313613\ttotal: 9.79s\tremaining: 559ms\n",
      "70:\tlearn: 1.0312963\ttotal: 9.94s\tremaining: 420ms\n",
      "71:\tlearn: 1.0312451\ttotal: 10.1s\tremaining: 280ms\n",
      "72:\tlearn: 1.0312052\ttotal: 10.2s\tremaining: 140ms\n",
      "73:\tlearn: 1.0311509\ttotal: 10.4s\tremaining: 0us\n",
      "Train: 1.0312568484216196, Test: 1.0344367712300173\n"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in splitter.split(X_train):\n",
    "    print(\"Starting\")\n",
    "    # Prepare data for LightGBM\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx].copy(), X_train.iloc[val_idx].copy()\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx].copy(), y_train.iloc[val_idx].copy()\n",
    "    \n",
    "    y_train_fold = np.log1p(y_train_fold)\n",
    "    y_val_fold = np.log1p(y_val_fold)\n",
    "    \n",
    "    categorical_features = [col for col in X_train_fold.columns if X_train_fold[col].dtype == \"category\"]\n",
    "    \n",
    "    full_pipeline = Pipeline([\n",
    "        (\"categorical_to_string\", CatogoricalUnknownTransformer()),\n",
    "        (\"model\",CatBoostRegressor(**catboost_params, cat_features=categorical_features))\n",
    "    ])\n",
    "    full_pipeline.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_train_fold)), np.expm1(y_train_fold))\n",
    "    val_score = root_mean_squared_log_error(np.expm1(full_pipeline.predict(X_val_fold)), np.expm1(y_val_fold))\n",
    "\n",
    "    print(f\"Train: {train_score}, Test: {val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Fitting catboost - Start 2024-12-29 02:10:21.378310\n",
      "Fitting catboost - End 2024-12-29 02:10:52.899307\n",
      "Fitting xgboost - Start 2024-12-29 02:10:53.141309\n",
      "Fitting xgboost - End 2024-12-29 02:10:56.184306\n",
      "Fitting xgboost_rf - Start 2024-12-29 02:10:56.184306\n",
      "Fitting xgboost_rf - End 2024-12-29 02:10:59.613306\n",
      "Fitting lgbm - Start 2024-12-29 02:10:59.613306\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4800\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 6.593848\n",
      "Fitting lgbm - End 2024-12-29 02:11:02.467308\n",
      "Fitting stack_model - Start 2024-12-29 02:11:02.467308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\insurance\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [02:11:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stack_model - End 2024-12-29 02:11:05.478308\n",
      "Train: 1.0220528175630508, Test: 1.0442864042947442\n",
      "Starting\n",
      "Fitting catboost - Start 2024-12-29 02:11:08.779307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 25\u001b[0m\n\u001b[0;32m     13\u001b[0m catboost_model \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     14\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_to_string\u001b[39m\u001b[38;5;124m\"\u001b[39m, CatogoricalUnknownTransformer()),\n\u001b[0;32m     15\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,CatBoostRegressor(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m                       learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     23\u001b[0m ])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting catboost - Start\u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[1;32m---> 25\u001b[0m \u001b[43mcatboost_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting catboost - End\u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m     28\u001b[0m linear_model_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     29\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m, build_preprocessing_pipeline(train_data\u001b[38;5;241m=\u001b[39mX_train_fold, imputation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m, binning_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopti-means\u001b[39m\u001b[38;5;124m\"\u001b[39m, categorical_to_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     30\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, LinearRegression())\n\u001b[0;32m     31\u001b[0m ])\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\catboost\\core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5874\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5875\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5876\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32md:\\Development\\insurance\\venv\\lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5017\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5066\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for train_idx, val_idx in splitter.split(X_train):\n",
    "    print(\"Starting\")\n",
    "    # Prepare data for LightGBM\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    y_train_fold = np.log1p(y_train_fold)\n",
    "    y_val_fold = np.log1p(y_val_fold)\n",
    "\n",
    "    categorical_features = [col for col in X_train_fold.columns if X_train_fold[col].dtype == \"category\"]\n",
    "        \n",
    "    catboost_model = Pipeline([\n",
    "        (\"categorical_to_string\", CatogoricalUnknownTransformer()),\n",
    "        (\"model\",CatBoostRegressor(**catboost_params, cat_features=categorical_features))\n",
    "    ])\n",
    "    print(\"Fitting catboost - Start\", datetime.now())\n",
    "    catboost_model.fit(X_train_fold, y_train_fold)\n",
    "    print(\"Fitting catboost - End\", datetime.now())\n",
    "    \n",
    "    linear_model_pipeline = Pipeline([\n",
    "        (\"preprocessing\", build_preprocessing_pipeline(train_data=X_train_fold, imputation_method=\"simple\", binning_method=\"opti-means\", categorical_to_string=True)),\n",
    "        (\"model\", LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    hist_pipeline = Pipeline([\n",
    "        (\"preprocessing\", build_preprocessing_pipeline(train_data=X_train_fold, imputation_method=\"simple\", categorical_to_string=True)),\n",
    "        (\"model\", HistGradientBoostingRegressor())\n",
    "    ])\n",
    "    \n",
    "    xgboost = xgb.XGBRegressor(enable_categorical=True, eta=0.1, colsample_bytree=0.9, n_estimators=100, device=\"cuda\", tree_method=\"hist\")\n",
    "    print(\"Fitting xgboost - Start\", datetime.now())\n",
    "    xgboost.fit(X_train_fold, y_train_fold)\n",
    "    print(\"Fitting xgboost - End\", datetime.now())\n",
    "    \n",
    "    xgboost_rf = xgb.XGBRFRegressor(enable_categorical=True, eta=0.1, colsample_bytree=0.9, n_estimators=100, device=\"cuda\", tree_method=\"hist\")\n",
    "    print(\"Fitting xgboost_rf - Start\", datetime.now())\n",
    "    xgboost_rf.fit(X_train_fold, y_train_fold)\n",
    "    print(\"Fitting xgboost_rf - End\", datetime.now())\n",
    "    \n",
    "    lgbm = LGBMRegressor(learning_rate=0.1, colsample_bytree=0.9, n_estimators=100)\n",
    "    print(\"Fitting lgbm - Start\", datetime.now())\n",
    "    lgbm.fit(X_train_fold, y_train_fold)\n",
    "    print(\"Fitting lgbm - End\", datetime.now())\n",
    "\n",
    "    stack_model = StackingRegressor(\n",
    "        estimators=[\n",
    "            #(\"linear_model\", linear_model_pipeline),\n",
    "            (\"xgboost\", xgboost),\n",
    "            (\"xgboost_rf\", xgboost_rf),\n",
    "            (\"lgbm\", lgbm),\n",
    "            (\"catboost\", catboost_model)\n",
    "        ],\n",
    "        #final_estimator=BayesianRidge(),\n",
    "        cv='prefit',\n",
    "        final_estimator=xgb.XGBRegressor(enable_categorical=True, eta=0.01, colsample_bytree=0.9, n_estimators=100, device=\"cuda\", tree_method=\"hist\"),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(\"Fitting stack_model - Start\", datetime.now())\n",
    "    stack_model.fit(X_train_fold, y_train_fold)\n",
    "    print(\"Fitting stack_model - End\", datetime.now())\n",
    "\n",
    "    train_score = root_mean_squared_log_error(np.expm1(stack_model.predict(X_train_fold)), np.expm1(y_train_fold))\n",
    "    val_score = root_mean_squared_log_error(np.expm1(stack_model.predict(X_val_fold)), np.expm1(y_val_fold))\n",
    "\n",
    "    print(f\"Train: {train_score}, Test: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.fit(treated_dataset, treated_dataset['Premium Amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = full_pipeline.predict(treated_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import rmsle_metric, source_range_root_mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_range_root_mean_squared_log_error(treated_dataset['Premium Amount'], res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(res).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mf.ImputationKernel(treated_dataset, num_datasets=1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optbinning import ContinuousOptimalBinning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ContinuousOptimalBinning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = ContinuousOptimalBinning(name=\"Premium Amount\", dtype=\"numerical\")\n",
    "binning.fit(treated_dataset[\"Credit Score\"], treated_dataset[\"Premium Amount\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning.binning_table.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "        'objective': 'regression',  # Use 'binary' for classification\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.02, 0.1),  # Fine-tuned range\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 1, 50),  # Wider range for more exploration\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 50),  # Allow deeper trees or unlimited depth (-1)\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 0.8),  # Sampling fraction of features\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.85, 1.0),  # Sampling fraction of data\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),  # Frequency of bagging\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),  # Minimum samples per leaf\n",
    "        'min_split_gain': trial.suggest_loguniform('min_split_gain', 1e-3, 0.1),  # Minimum gain for a split\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-3, 10),  # Minimum weight for children\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 10),  # L1 regularization\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 10),  # L2 regularization\n",
    "        'cat_smooth': trial.suggest_uniform('cat_smooth', 5, 30),  # Smoothing for categorical features\n",
    "        'max_bin': trial.suggest_int('max_bin', 150, 200),  # Granularity of feature splits\n",
    "        'verbose': -1,  # Suppress output\n",
    "        'random_state': 41,  # Reproducibility\n",
    "    }\n",
    "    print(param_grid)\n",
    "    \n",
    "    # Define a 60-40 split\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.4, random_state=1)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    for train_idx, val_idx in splitter.split(X_train_fixed):\n",
    "        # Prepare data for LightGBM\n",
    "        X_train_fold, X_val_fold = X_train_fixed.iloc[train_idx], X_train_fixed.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        train_data_lgbm = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "        val_data_lgbm = lgb.Dataset(X_val_fold, label=y_val_fold)\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            param_grid,\n",
    "            train_data_lgbm,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[train_data_lgbm, val_data_lgbm],\n",
    "            valid_names=[\"train\", \"validation\"],\n",
    "            feval=rmsle_metric,    # Custom RMSLE metric\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=10),\n",
    "                lgb.log_evaluation(period=5),  # Optional: Logs evaluation every 10 rounds\n",
    "            ]\n",
    "        )\n",
    "        results.append(model.best_score['validation']['rmsle'])\n",
    "        \n",
    "    # Use validation score\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
